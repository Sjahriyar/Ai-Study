{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is DeepLearning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a computer technique to extract and transform data–-with use cases ranging from human speech recognition to animal imagery classification–-by using multiple layers of neural networks. Each of these layers takes its inputs from previous layers and progressively refines them. The layers are trained by algorithms that minimize their errors and improve their accuracy. In this way, the network learns to perform a specified task.\n",
    "\n",
    "> : A PhD is definitely not required. All that matters is a deep understanding of AI & ability to implement NNs in a way that is actually useful (latter point is what’s truly hard). Don’t care if you even graduated high school."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Machine learning is, like regular programming, a way to get computers to complete a specific task. But how would we use regular programming to do what we just did in the last section: recognize dogs versus cats in photos? We would have to write down for the computer the exact steps necessary to complete the task.\n",
    "\n",
    "Normally, it's easy enough for us to write down the steps to complete a task when we're writing a program. We just think about the steps we'd take if we had to do the task by hand, and then we translate them into code. For instance, we can write a function that sorts a list. In general, we'd write a function that looks something like (where *inputs* might be an unsorted list, and *results* a sorted list).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"289pt\" height=\"58pt\"\n",
       " viewBox=\"0.00 0.00 288.82 58.40\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 54.4)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-54.4 284.82,-54.4 284.82,4 -4,4\"/>\n",
       "<!-- program -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>program</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"174.88,-50.4 106.88,-50.4 102.88,-46.4 102.88,0 170.88,0 174.88,-4 174.88,-50.4\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"170.88,-46.4 102.88,-46.4\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"170.88,-46.4 170.88,0\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"170.88,-46.4 174.88,-50.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.88\" y=\"-20.15\" font-family=\"Times,serif\" font-size=\"14.00\">program</text>\n",
       "</g>\n",
       "<!-- results -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>results</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"245.85\" cy=\"-25.2\" rx=\"34.97\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"245.85\" y=\"-20.15\" font-family=\"Times,serif\" font-size=\"14.00\">results</text>\n",
       "</g>\n",
       "<!-- program&#45;&gt;results -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>program&#45;&gt;results</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M175.12,-25.2C182.9,-25.2 191.24,-25.2 199.35,-25.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.07,-28.7 209.07,-25.2 199.07,-21.7 199.07,-28.7\"/>\n",
       "</g>\n",
       "<!-- inputs -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>inputs</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"33.44\" cy=\"-25.2\" rx=\"33.44\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"33.44\" y=\"-20.15\" font-family=\"Times,serif\" font-size=\"14.00\">inputs</text>\n",
       "</g>\n",
       "<!-- inputs&#45;&gt;program -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inputs&#45;&gt;program</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.17,-25.2C74.81,-25.2 83.1,-25.2 91.21,-25.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.98,-28.7 100.98,-25.2 90.98,-21.7 90.98,-28.7\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x35754d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#caption A traditional program\n",
    "#id basic_program\n",
    "#alt Pipeline inputs, program, results\n",
    "gv('''program[shape=box3d width=1 height=0.7]\n",
    "inputs->program->results''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for recognizing objects in a photo that's a bit tricky; what *are* the steps we take when we recognize an object in a picture? We really don't know, since it all happens in our brain without us being consciously aware of it!\n",
    "\n",
    "Right back at the dawn of computing, in 1949, an IBM researcher named Arthur Samuel started working on a different way to get computers to complete tasks, which he called *machine learning*. In his classic 1962 essay \"Artificial Intelligence: A Frontier of Automation\", he wrote:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : Programming a computer for such computations is, at best, a difficult task, not primarily because of any inherent complexity in the computer itself but, rather, because of the need to spell out every minute step of the process in the most exasperating detail. Computers, as any programmer will tell you, are giant morons, not giant brains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "His basic idea was this: instead of telling the computer the exact steps required to solve a problem, show it examples of the problem to solve, and let it figure out how to solve it itself. This turned out to be very effective: by 1961 his checkers-playing program had learned so much that it beat the Connecticut state champion! Here's how he described his idea (from the same essay as above):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> : Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of powerful concepts embedded in this short statement: \n",
    "\n",
    "- The idea of a \"weight assignment\" \n",
    "- The fact that every weight assignment has some \"actual performance\"\n",
    "- The requirement that there be an \"automatic means\" of testing that performance,  \n",
    "- The need for a \"mechanism\" (i.e., another automatic process) for improving the performance by changing the weight assignments\n",
    "\n",
    "Let us take these concepts one by one, in order to understand how they fit together in practice. First, we need to understand what Samuel means by a *weight assignment*.\n",
    "\n",
    "Weights are just variables, and a weight assignment is a particular choice of values for those variables. The program's inputs are values that it processes in order to produce its results—for instance, taking image pixels as inputs, and returning the classification \"dog\" as a result. The program's weight assignments are other values that define how the program will operate.\n",
    "\n",
    "Since they will affect the program they are in a sense another kind of input, so we will update our basic picture in <<basic_program>> and replace it with <<weight_assignment>> in order to take this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"301pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 301.11 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-94 297.11,-94 297.11,4 -4,4\"/>\n",
       "<!-- model -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"187.16,-70.2 119.16,-70.2 115.16,-66.2 115.16,-19.8 183.16,-19.8 187.16,-23.8 187.16,-70.2\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"183.16,-66.2 115.16,-66.2\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"183.16,-66.2 183.16,-19.8\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"183.16,-66.2 187.16,-70.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.16\" y=\"-39.95\" font-family=\"Times,serif\" font-size=\"14.00\">model</text>\n",
       "</g>\n",
       "<!-- results -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>results</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"258.13\" cy=\"-45\" rx=\"34.97\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"258.13\" y=\"-39.95\" font-family=\"Times,serif\" font-size=\"14.00\">results</text>\n",
       "</g>\n",
       "<!-- model&#45;&gt;results -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>model&#45;&gt;results</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M187.41,-45C195.18,-45 203.52,-45 211.63,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"211.36,-48.5 221.36,-45 211.36,-41.5 211.36,-48.5\"/>\n",
       "</g>\n",
       "<!-- inputs -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>inputs</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"39.58\" cy=\"-72\" rx=\"33.44\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"39.58\" y=\"-66.95\" font-family=\"Times,serif\" font-size=\"14.00\">inputs</text>\n",
       "</g>\n",
       "<!-- inputs&#45;&gt;model -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inputs&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M70.52,-64.62C80.79,-62.09 92.53,-59.2 103.79,-56.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"104.47,-59.86 113.34,-54.07 102.79,-53.07 104.47,-59.86\"/>\n",
       "</g>\n",
       "<!-- weights -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>weights</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"39.58\" cy=\"-18\" rx=\"39.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"39.58\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">weights</text>\n",
       "</g>\n",
       "<!-- weights&#45;&gt;model -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>weights&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74.95,-26.47C84.06,-28.71 94.06,-31.18 103.7,-33.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"102.76,-36.93 113.31,-35.92 104.44,-30.13 102.76,-36.93\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x358d15690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#caption A program using weight assignment\n",
    "#id weight_assignment\n",
    "gv('''model[shape=box3d width=1 height=0.7]\n",
    "inputs->model->results; weights->model''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've changed the name of our box from *program* to *model*. This is to follow modern terminology and to reflect that the *model* is a special kind of program: it's one that can do *many different things*, depending on the *weights*. It can be implemented in many different ways. For instance, in Samuel's checkers program, different values of the weights would result in different checkers-playing strategies. \n",
    "\n",
    "(By the way, what Samuel called \"weights\" are most generally referred to as model *parameters* these days, in case you have encountered that term. The term *weights* is reserved for a particular type of model parameter.)\n",
    "\n",
    "Next, Samuel said we need an *automatic means of testing the effectiveness of any current weight assignment in terms of actual performance*. In the case of his checkers program, the \"actual performance\" of a model would be how well it plays. And you could automatically test the performance of two models by setting them to play against each other, and seeing which one usually wins.\n",
    "\n",
    "Finally, he says we need *a mechanism for altering the weight assignment so as to maximize the performance*. For instance, we could look at the difference in weights between the winning model and the losing model, and adjust the weights a little further in the winning direction.\n",
    "\n",
    "We can now see why he said that such a procedure *could be made entirely automatic and... a machine so programmed would \"learn\" from its experience*. Learning would become entirely automatic when the adjustment of the weights was also automatic—when instead of us improving a model by adjusting its weights manually, we relied on an automated mechanism that produced adjustments based on performance.\n",
    "\n",
    "<<training_loop>> shows the full picture of Samuel's idea of training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"491pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 491.12 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-94 487.12,-94 487.12,4 -4,4\"/>\n",
       "<!-- model -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"188.16,-78.2 120.16,-78.2 116.16,-74.2 116.16,-27.8 184.16,-27.8 188.16,-31.8 188.16,-78.2\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"184.16,-74.2 116.16,-74.2\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"184.16,-74.2 184.16,-27.8\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"184.16,-74.2 188.16,-78.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.16\" y=\"-47.95\" font-family=\"Times,serif\" font-size=\"14.00\">model</text>\n",
       "</g>\n",
       "<!-- results -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>results</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"295.13\" cy=\"-53\" rx=\"34.97\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"295.13\" y=\"-47.95\" font-family=\"Times,serif\" font-size=\"14.00\">results</text>\n",
       "</g>\n",
       "<!-- model&#45;&gt;results -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>model&#45;&gt;results</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M188.37,-53C206.55,-53 228.96,-53 248.46,-53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"248.34,-56.5 258.34,-53 248.34,-49.5 248.34,-56.5\"/>\n",
       "</g>\n",
       "<!-- inputs -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>inputs</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"39.58\" cy=\"-72\" rx=\"33.44\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"39.58\" y=\"-66.95\" font-family=\"Times,serif\" font-size=\"14.00\">inputs</text>\n",
       "</g>\n",
       "<!-- inputs&#45;&gt;model -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inputs&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.97,-66.61C82.11,-64.86 93.58,-62.89 104.58,-61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"105.04,-64.48 114.3,-59.33 103.86,-57.58 105.04,-64.48\"/>\n",
       "</g>\n",
       "<!-- performance -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>performance</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"425.11\" cy=\"-53\" rx=\"58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"425.11\" y=\"-47.95\" font-family=\"Times,serif\" font-size=\"14.00\">performance</text>\n",
       "</g>\n",
       "<!-- results&#45;&gt;performance -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>results&#45;&gt;performance</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.43,-53C338.25,-53 346.84,-53 355.54,-53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"355.47,-56.5 365.47,-53 355.47,-49.5 355.47,-56.5\"/>\n",
       "</g>\n",
       "<!-- weights -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>weights</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"39.58\" cy=\"-18\" rx=\"39.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"39.58\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">weights</text>\n",
       "</g>\n",
       "<!-- weights&#45;&gt;model -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>weights&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M72.86,-28.22C83,-31.43 94.39,-35.03 105.28,-38.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"103.9,-41.71 114.49,-41.39 106.02,-35.04 103.9,-41.71\"/>\n",
       "</g>\n",
       "<!-- performance&#45;&gt;weights -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>performance&#45;&gt;weights</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.24,-39.66C368.48,-34.47 348.53,-29.06 330.11,-26 247.04,-12.21 148.75,-13.04 90.71,-15.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.66,-11.76 80.82,-15.67 90.95,-18.75 90.66,-11.76\"/>\n",
       "<text text-anchor=\"middle\" x=\"224.16\" y=\"-19.67\" font-family=\"Times,serif\" font-size=\"14.00\">update</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x35b0c0650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#caption Training a machine learning model\n",
    "#id training_loop\n",
    "#alt The basic training loop\n",
    "gv('''ordering=in\n",
    "model[shape=box3d width=1 height=0.7]\n",
    "inputs->model->results; weights->model; results->performance\n",
    "performance->weights[constraint=false label=update]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the distinction between the model's *results*  (e.g., the moves in a checkers game) and its *performance* (e.g., whether it wins the game, or how quickly it wins). \n",
    "\n",
    "Also note that once the model is trained—that is, once we've chosen our final, best, favorite weight assignment—then we can think of the weights as being *part of the model*, since we're not varying them any more.\n",
    "\n",
    "Therefore, actually *using* a model after it's trained looks like <<using_model>>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"289pt\" height=\"58pt\"\n",
       " viewBox=\"0.00 0.00 288.82 58.40\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 54.4)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-54.4 284.82,-54.4 284.82,4 -4,4\"/>\n",
       "<!-- model -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"174.88,-50.4 106.88,-50.4 102.88,-46.4 102.88,0 170.88,0 174.88,-4 174.88,-50.4\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"170.88,-46.4 102.88,-46.4\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"170.88,-46.4 170.88,0\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"170.88,-46.4 174.88,-50.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.88\" y=\"-20.15\" font-family=\"Times,serif\" font-size=\"14.00\">model</text>\n",
       "</g>\n",
       "<!-- results -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>results</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"245.85\" cy=\"-25.2\" rx=\"34.97\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"245.85\" y=\"-20.15\" font-family=\"Times,serif\" font-size=\"14.00\">results</text>\n",
       "</g>\n",
       "<!-- model&#45;&gt;results -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>model&#45;&gt;results</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M175.12,-25.2C182.9,-25.2 191.24,-25.2 199.35,-25.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.07,-28.7 209.07,-25.2 199.07,-21.7 199.07,-28.7\"/>\n",
       "</g>\n",
       "<!-- inputs -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>inputs</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"33.44\" cy=\"-25.2\" rx=\"33.44\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"33.44\" y=\"-20.15\" font-family=\"Times,serif\" font-size=\"14.00\">inputs</text>\n",
       "</g>\n",
       "<!-- inputs&#45;&gt;model -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inputs&#45;&gt;model</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.17,-25.2C74.81,-25.2 83.1,-25.2 91.21,-25.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.98,-28.7 100.98,-25.2 90.98,-21.7 90.98,-28.7\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x35b09c610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "#caption Using a trained model as a program\n",
    "#id using_model\n",
    "gv('''model[shape=box3d width=1 height=0.7]\n",
    "inputs->model->results''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks identical to our original diagram in <<basic_program>>, just with the word *program* replaced with *model*. This is an important insight: *a trained model can be treated just like a regular computer program*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Machine Learning: The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is a Neural Network?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too hard to imagine what the model might look like for a checkers program. There might be a range of checkers strategies encoded, and some kind of search mechanism, and then the weights could vary how strategies are selected, what parts of the board are focused on during a search, and so forth. But it's not at all obvious what the model might look like for an image recognition program, or for understanding text, or for many other interesting problems we might imagine.\n",
    "\n",
    "What we would like is some kind of function that is so flexible that it could be used to solve any given problem, just by varying its weights. Amazingly enough, this function actually exists! It's the neural network, which we already discussed. That is, if you regard a neural network as a mathematical function, it turns out to be a function which is extremely flexible depending on its weights. A mathematical proof called the *universal approximation theorem* shows that this function can solve any problem to any level of accuracy, in theory. The fact that neural networks are so flexible means that, in practice, they are often a suitable kind of model, and you can focus your effort on the process of training them—that is, of finding good weight assignments.\n",
    "\n",
    "But what about that process?  One could imagine that you might need to find a new \"mechanism\" for automatically updating weights for every problem. This would be laborious. What we'd like here as well is a completely general way to update the weights of a neural network, to make it improve at any given task. Conveniently, this also exists!\n",
    "\n",
    "This is called *stochastic gradient descent* (SGD). We'll see how neural networks and SGD work in detail in <<chapter_mnist_basics>>, as well as explaining the universal approximation theorem. For now, however, we will instead use Samuel's own words: *We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: Don't worry, neither SGD nor neural nets are mathematically complex. Both nearly entirely rely on addition and multiplication to do their work (but they do a _lot_ of addition and multiplication!). The main reaction we hear from students when they see the details is: \"Is that all it is?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, to recap, a neural network is a particular kind of machine learning model, which fits right in to Samuel's original conception. Neural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. This is powerful, because stochastic gradient descent provides us a way to find those weight values automatically.\n",
    "\n",
    "Having zoomed out, let's now zoom back in and revisit our image classification problem using Samuel's framework.\n",
    "\n",
    "Our inputs are the images. Our weights are the weights in the neural net. Our model is a neural net. Our results are the values that are calculated by the neural net, like \"dog\" or \"cat.\"\n",
    "\n",
    "What about the next piece, an *automatic means of testing the effectiveness of any current weight assignment in terms of actual performance*? Determining \"actual performance\" is easy enough: we can simply define our model's performance as its accuracy at predicting the correct answers.\n",
    "\n",
    "Putting this all together, and assuming that SGD is our mechanism for updating the weight assignments, we can see how our image classifier is a machine learning model, much like Samuel envisioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"uni_apprx_theo\"></a>\n",
    "# **What Is the Universal Approximation Theorem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/the_universal-approximation-theorem.png\" width=600 height=400/>\n",
    "\n",
    "The **Universal Approximation Theorem** says this:\n",
    "\n",
    "> A neural network with just **one hidden layer** (and enough neurons in that layer) can approximate **any function**, no matter how complicated, as long as the function is continuous.\n",
    "\n",
    "In plain English:\n",
    "- If you have a problem where you want to map inputs to outputs (like predicting house prices, recognizing images, etc.), a neural network can learn to do it.\n",
    "- It doesn’t matter how crazy or complex the relationship between inputs and outputs is—given enough neurons, the neural network can figure it out.\n",
    "\n",
    "### **Why Is This Important?**\n",
    "\n",
    "It means neural networks are **super flexible**. You don’t need to design a specific formula or rule for every problem. Instead, you can throw data at a neural network, and it will \"learn\" the pattern by itself.\n",
    "\n",
    "### **An Analogy**\n",
    "\n",
    "Imagine you’re trying to draw any shape—say, a circle, a wavy line, or even a weird squiggle. Now imagine you have a bunch of simple building blocks (like LEGO pieces). The Universal Approximation Theorem says:\n",
    "\n",
    "> With enough LEGO pieces, you can build **any shape** you want.\n",
    "\n",
    "In this analogy:\n",
    "- The \"shape\" is the function you’re trying to approximate (e.g., predicting house prices based on features like size, location, etc.).\n",
    "- The \"LEGO pieces\" are the neurons in the neural network.\n",
    "\n",
    "### **How Does It Work?**\n",
    "\n",
    "1. **Neurons Are Like Little Functions**:\n",
    "   - Each neuron in the hidden layer performs a simple calculation (like multiplying inputs by weights and applying an activation function).\n",
    "\n",
    "2. **Combine Neurons to Approximate Complex Patterns**:\n",
    "   - By combining many neurons together, the network can create more and more complex patterns.\n",
    "   - Think of it like adding up lots of simple curves to make a complicated curve.\n",
    "\n",
    "3. **One Hidden Layer Is Enough**:\n",
    "   - The theorem says you only need **one hidden layer** to approximate any function. However, in practice, deeper networks (with multiple layers) often work better and are more efficient.\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "While the theorem is powerful, there are some important caveats:\n",
    "1. **\"Enough Neurons\" Can Be a Lot**:\n",
    "   - The theorem doesn’t say how many neurons you’ll need. For very complex problems, you might need thousands or millions of neurons.\n",
    "\n",
    "2. **Training Is Hard**:\n",
    "   - Even if a neural network *can* approximate any function, actually training it to do so might be tricky. You need good data, a good optimization algorithm, and lots of computational power.\n",
    "\n",
    "3. **Continuous Functions Only**:\n",
    "   - The theorem applies to continuous functions (smooth, unbroken patterns). If your problem involves sudden jumps or discontinuities, the neural network might struggle.\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "1. **What It Says**:\n",
    "   - A neural network with one hidden layer can learn to approximate almost any function.\n",
    "\n",
    "2. **Why It Matters**:\n",
    "   - You don’t need to design custom solutions for every problem. Neural networks can \"figure it out\" for you.\n",
    "\n",
    "3. **Simple Analogy**:\n",
    "   - Think of neurons as LEGO pieces. With enough of them, you can build any shape (or approximate any function).\n",
    "\n",
    "4. **Real-World Use**:\n",
    "   - This is why neural networks are used everywhere—from image recognition to language translation to self-driving cars.\n",
    "\n",
    "### **Final Answer**\n",
    "\n",
    "The **Universal Approximation Theorem** says that a neural network with one hidden layer can learn to mimic almost any function, no matter how complicated, as long as you give it enough neurons. It’s like saying: \"With enough LEGO pieces, you can build anything!\" 😊\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are metrics? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are **tools or numbers we use to measure how well an AI model is performing**. Think of them like a report card for your AI system—they tell you whether the model is doing a good job or needs improvement.\n",
    "\n",
    "In AI, metrics help us evaluate the quality of predictions made by the model compared to the actual correct answers (called \"ground truth\"). Different tasks (like classification, regression, or translation) use different metrics depending on what’s being measured.\n",
    "\n",
    "### Why Are Metrics Important?\n",
    "\n",
    "1. **To Evaluate Performance**: Metrics tell us if the model is accurate, fast, or reliable.\n",
    "2. **To Compare Models**: If you have two models, metrics help you decide which one works better.\n",
    "3. **To Improve Models**: Metrics highlight weaknesses, so you know what to fix during training.\n",
    "\n",
    "### Common Types of Metrics\n",
    "\n",
    "Here’s a breakdown of metrics used for different AI tasks:\n",
    "\n",
    "#### 1. **Classification Metrics**  \n",
    "   Used when the model predicts categories or labels (e.g., \"spam\" vs. \"not spam\").\n",
    "\n",
    "   - **Accuracy**:  \n",
    "     - Measures the percentage of correct predictions out of all predictions.\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "       $$\n",
    "     - Example: If a model gets 90 out of 100 predictions right, accuracy is 90%.\n",
    "\n",
    "   - **Precision**:  \n",
    "     - Measures how many of the predicted positives are actually correct.\n",
    "     - Useful when false positives are costly (e.g., flagging normal emails as spam).\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
    "       $$\n",
    "\n",
    "   - **Recall (Sensitivity)**:  \n",
    "     - Measures how many of the actual positives the model caught.\n",
    "     - Useful when missing positives is bad (e.g., detecting diseases).\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "       $$\n",
    "\n",
    "   - **F1-Score**:  \n",
    "     - A balance between precision and recall (useful when you want both to be good).\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "       $$\n",
    "\n",
    "#### 2. **Regression Metrics**  \n",
    "   Used when the model predicts continuous values (e.g., house prices, temperature).\n",
    "\n",
    "   - **Mean Absolute Error (MAE)**:  \n",
    "     - Measures the average difference between predicted and actual values.\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{MAE} = \\frac{\\sum |y_{\\text{predicted}} - y_{\\text{actual}}|}{n}\n",
    "       $$\n",
    "     - Example: If predictions are off by 2, 3, and 5 units, MAE = $(2 + 3 + 5) / 3 = 3.33$.\n",
    "\n",
    "   - **Mean Squared Error (MSE)**:  \n",
    "     - Similar to MAE but squares the differences, punishing larger errors more.\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{MSE} = \\frac{\\sum (y_{\\text{predicted}} - y_{\\text{actual}})^2}{n}\n",
    "       $$\n",
    "\n",
    "   - **Root Mean Squared Error (RMSE)**:  \n",
    "     - The square root of MSE, making it easier to interpret (same units as the data).\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "       $$\n",
    "\n",
    "#### 3. **Clustering Metrics**  \n",
    "   Used when grouping similar data points together (e.g., customer segmentation).\n",
    "\n",
    "   - **Silhouette Score**:  \n",
    "     - Measures how well each data point fits into its cluster compared to other clusters.\n",
    "     - Ranges from -1 (bad clustering) to +1 (good clustering).\n",
    "\n",
    "   - **Davies-Bouldin Index**:  \n",
    "     - Measures the average similarity between clusters (lower is better).\n",
    "\n",
    "#### 4. **Text Generation Metrics**  \n",
    "   Used for tasks like translation, summarization, or chatbots.\n",
    "\n",
    "   - **BLEU (Bilingual Evaluation Understudy)**:  \n",
    "     - Compares generated text to reference text, measuring word overlap.\n",
    "     - Higher scores mean closer matches.\n",
    "\n",
    "   - **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**:  \n",
    "     - Measures how much of the reference text is captured in the generated text.\n",
    "     - Commonly used for summarization.\n",
    "\n",
    "### Real-Life Analogy\n",
    "\n",
    "Think of metrics like checking the performance of a car:\n",
    "- **Accuracy**: How often the car reaches the destination without mistakes.\n",
    "- **Precision**: How close the car stays to the correct lane.\n",
    "- **Recall**: How well the car avoids missing turns.\n",
    "- **MAE/MSE**: How far the car deviates from the ideal speed or route.\n",
    "\n",
    "### Choosing the Right Metric\n",
    "\n",
    "The metric you choose depends on the task and what matters most:\n",
    "- If you care about overall correctness → Use **accuracy**.\n",
    "- If false positives matter → Use **precision**.\n",
    "- If missing important cases matters → Use **recall**.\n",
    "- If you want a balance → Use **F1-score**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Label?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we're trying to predict, such as \"dog\" or \"cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Transfer Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using a pretrained model for a task different to what it was originally trained for is known as *transfer learning*. Unfortunately, because transfer learning is so under-studied, few domains have pretrained models available. For instance, there are currently few pretrained models available in medicine, making transfer learning challenging to use in that domain. In addition, it is not yet well understood how to use transfer learning for tasks such as time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is CNN (Convolutional Neural Network)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the current state-of-the-art approach to creating computer vision models. Their structure is inspired by how the human vision system works.\n",
    "Convolutional neural network, is a type of neural network that works particularly well for computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Architecture?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture only describes a *template* for a mathematical function; it doesn't actually do anything until we provide values for the millions of parameters it contains. Picking an architecture isn't a very important part of the deep learning process. It's something that academics love to talk about, but in practice it is unlikely to be something you need to spend much time on. There are some standard architectures that work most of the time, and in this case we're using one called _ResNet_ that we'll be talking a lot about during the book; it is both fast and accurate for many datasets and problems. The `34` in `resnet34` refers to the number of layers in this variant of the architecture (other options are `18`, `50`, `101`, and `152`). Models using architectures with more layers take longer to train, and are more prone to overfitting (i.e. you can't train them for as many epochs before the accuracy on the validation set starts getting worse). On the other hand, when using more data, they can be quite a bit more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of the architecture with a particular set of parameters.\n",
    "Every model starts with a choice of architecture, a general template for how that kind of model works internally. The process of training (or fitting) the model is the process of finding a set of parameter values (or weights) that specialize that general architecture into a model that works well for our particular kind of data. In order to define how well a model does on a single prediction, we need to define a loss function, which determines how we score a prediction as good or bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Paramters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the model that change what task it can do, and are updated through model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Stepping paramter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stepping parameters** simply means **updating the values of the model's parameters (like weights and biases)** to make the model better at its task. \n",
    "\n",
    "Here’s how it works in simple terms:\n",
    "\n",
    "1. **Why Step?**\n",
    "   - The goal is to minimize the **loss** (the error between the model's predictions and the actual data).\n",
    "   - To do this, we adjust the parameters step by step until the model improves.\n",
    "\n",
    "2. **How Do We Step?**\n",
    "   - Using the **gradient** (which tells us how much each parameter affects the loss), we nudge the parameters in the right direction.\n",
    "   - The size of the step is controlled by the **learning rate**.\n",
    "\n",
    "3. **What Happens During a Step?**\n",
    "   - For each parameter (e.g., weight $ w $ or bias $ b $):\n",
    "     $$\n",
    "     \\text{New Parameter} = \\text{Old Parameter} - \\text{Learning Rate} \\cdot \\text{Gradient}.\n",
    "     $$\n",
    "   - If the gradient is positive, the parameter decreases. If the gradient is negative, the parameter increases.\n",
    "\n",
    "4. **Why Call It \"Stepping\"?**\n",
    "   - Think of it like walking downhill: Each step moves you closer to the bottom (the minimum loss). You take small steps (controlled by the learning rate) based on the slope (gradient).\n",
    "\n",
    "In short: **Stepping parameters = Updating weights and biases to reduce the loss and improve the model.** 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Learning Rate (LR)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **learning rate** is a small number that controls **how big of a step** we take when updating the model's parameters during training. \n",
    "\n",
    "- If the learning rate is **too small**, the model learns very slowly.\n",
    "- If the learning rate is **too large**, the model might overshoot the optimal solution.\n",
    "\n",
    "In short:  \n",
    "**Learning rate = Step size for updating parameters to minimize the loss.** 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"activation-function\"></a>\n",
    "## What is Activation Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/activation_function_2.avif\" width=450 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An activation function is a mathematical function applied to the output of a neuron. It introduces non-linearity into the model, allowing the network to learn and represent complex patterns in the data. Without this non-linearity feature, a neural network would behave like a linear regression model, no matter how many layers it has.\n",
    "\n",
    "Activation function decides whether a neuron should be activated by calculating the weighted sum of inputs and adding a bias term. This helps the model make complex decisions and predictions by introducing non-linearities to the output of each neuron.\n",
    "\n",
    "### Introducing Non-Linearity in Neural Network\n",
    "Non-linearity means that the relationship between input and output is not a straight line. In simple terms, the output does not change proportionally with the input. A common choice is the ReLU function, defined as $σ(x)=max(0,x)$\n",
    "\n",
    "Imagine you want to classify apples and bananas based on their shape and color.\n",
    "\n",
    "If we use a linear function, it can only separate them using a straight line.\n",
    "But real-world data is often more complex (e.g., overlapping colors, different lighting).\n",
    "By adding a non-linear activation function (like ReLU, Sigmoid, or Tanh), the network can create curved decision boundaries to separate them correcty.\n",
    "\n",
    "### Why is Non-Linearity Important in Neural Networks?\n",
    "Neural networks consist of neurons that operate using weights, biases, and activation functions.\n",
    "\n",
    "In the learning process, these weights and biases are updated based on the error produced at the output—a process known as backpropagation. Activation functions enable backpropagation by providing gradients that are essential for updating the weights and biases.\n",
    "\n",
    "Without non-linearity, even deep networks would be limited to solving only simple, linearly separable problems. Activation functions empower neural networks to model highly complex data distributions and solve advanced deep learning tasks. Adding non-linear activation functions introduce flexibility and enable the network to learn more complex and abstract patterns from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **How Many Types Are There?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/activation_functions.avif\" width=600 height=500 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several common types of activation functions, but here are the most widely used ones:\n",
    "\n",
    "#### 1. **Sigmoid**\n",
    "   - Formula: $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "   - Output: Between `0` and `1`.\n",
    "   - Use Case: Binary classification (e.g., predicting probabilities).\n",
    "   - Example:\n",
    "     ```python\n",
    "     preds = xb.sigmoid()\n",
    "     ```\n",
    "\n",
    "#### 2. **ReLU (Rectified Linear Unit)**\n",
    "   - Formula: $ \\text{ReLU}(x) = \\max(0, x) $\n",
    "   - Output: `0` for negative inputs, and the input itself for positive inputs.\n",
    "   - Use Case: Hidden layers in deep neural networks (fast and effective).\n",
    "   - Example:\n",
    "     ```python\n",
    "     preds = torch.relu(xb)\n",
    "     ```\n",
    "\n",
    "#### 3. **Tanh (Hyperbolic Tangent)**\n",
    "   - Formula: $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "   - Output: Between `-1` and `1`.\n",
    "   - Use Case: Similar to sigmoid, but outputs are centered around `0`, which can help with training stability.\n",
    "\n",
    "#### 4. **Softmax**\n",
    "   - Formula: $ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $\n",
    "   - Output: Probabilities that sum to `1`.\n",
    "   - Use Case: Multi-class classification (e.g., predicting one of 10 digits in MNIST).\n",
    "\n",
    "### **Why Do We Need Activation Functions?**\n",
    "\n",
    "1. **Introduce Non-Linearity**:\n",
    "   - Without activation functions, every layer in a neural network would just perform a linear transformation, making the entire network equivalent to a single linear model.\n",
    "\n",
    "2. **Enable Complex Learning**:\n",
    "   - Activation functions allow the network to learn non-linear relationships in the data, which is crucial for solving real-world problems like image recognition, language processing, etc.\n",
    "\n",
    "3. **Output Constraints**:\n",
    "   - Some activation functions (like sigmoid and softmax) constrain the output to specific ranges (e.g., probabilities between `0` and `1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What is NumPy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy, short for Numerical Python, has long been a cornerstone of numerical computing in Python. It provides the data structures, algorithms, and library glue needed for most scientific applications involving numerical data in Python. NumPy contains, among other things:\n",
    "\n",
    "A fast and efficient multidimensional array object ndarray\n",
    "\n",
    "Functions for performing element-wise computations with arrays or mathematical operations between arrays\n",
    "\n",
    "Tools for reading and writing array-based datasets to disk\n",
    "\n",
    "Linear algebra operations, Fourier transform, and random number generation\n",
    "\n",
    "A mature C API to enable Python extensions and native C or C++ code to access NumPy's data structures and computational facilities\n",
    "\n",
    "Beyond the fast array-processing capabilities that NumPy adds to Python, one of its primary uses in data analysis is as a container for data to be passed between algorithms and libraries. For numerical data, NumPy arrays are more efficient for storing and manipulating data than the other built-in Python data structures. Also, libraries written in a lower-level language, such as C or FORTRAN, can operate on the data stored in a NumPy array without copying data into some other memory representation. Thus, many numerical computing tools for Python either assume NumPy arrays as a primary data structure or else target interoperability with NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What is Pandas?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas provides high-level data structures and functions designed to make working with structured or tabular data intuitive and flexible. Since its emergence in 2010, it has helped enable Python to be a powerful and productive data analysis environment. The primary objects in pandas that will be used in this book are the DataFrame, a tabular, column-oriented data structure with both row and column labels, and the Series, a one-dimensional labeled array object.\n",
    "\n",
    "pandas blends the array-computing ideas of NumPy with the kinds of data manipulation capabilities found in spreadsheets and relational databases (such as SQL). It provides convenient indexing functionality to enable you to reshape, slice and dice, perform aggregations, and select subsets of data. Since data manipulation, preparation, and cleaning are such important skills in data analysis, pandas is one of the primary focuses of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What is Matplot?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matplotlib is the most popular Python library for producing plots and other two-dimensional data visualizations. It was originally created by John D. Hunter and is now maintained by a large team of developers. It is designed for creating plots suitable for publication. While there are other visualization libraries available to Python programmers, matplotlib is still widely used and integrates reasonably well with the rest of the ecosystem. I think it is a safe choice as a default visualization tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Fine-Tuning (Fitting the Model)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the training process go faster, we might start with a pretrained model—a model that has already been trained on someone else's data. We can then adapt it to our data by training it a bit more on our data, a process called fine-tuning.\n",
    "This is the key to deep learning—determining how to fit the parameters of a model to get it to solve your problem. In order to fit a model, we have to provide at least one piece of information: how many times to look at each image **(known as number of *epochs*)**. The number of epochs you select will largely depend on how much time you have available, and how long you find it takes in practice to fit your model. If you select a number that is too small, you can always train for more epochs later.\n",
    "\n",
    "But why is the method called `fine_tune`, and not `fit`? fastai actually *does* have a method called `fit`, which does indeed fit a model (i.e. look at images in the training set multiple times, each time updating the parameters to make the predictions closer and closer to the target labels). But in this case, we've started with a pretrained model, and we don't want to throw away all those capabilities that it already has. There are some important tricks to adapt a pretrained model for a new dataset—a process called *fine-tuning*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Fine-tuning: A transfer learning technique where the parameters (wights) of a pretrained model are updated by training for additional epochs using a different task to that used for pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a model, a key concern is to ensure that our model generalizes—that is, that it learns general lessons from our data which also apply to new items it will encounter, so that it can make good predictions on those items. The risk is that if we train our model badly, instead of learning general lessons it effectively memorizes what it has already seen, and then it will make poor predictions about new images. Such a failure is called overfitting.\n",
    " In order to avoid this, we always divide our data into two parts, the training set and the validation set. We train the model by showing it only the training set and then we evaluate how well the model is doing by seeing how well it performs on items from the validation set. In this way, we check if the lessons the model learns from the training set are lessons that generalize to the validation set. In order for a person to assess how well the model is doing on the validation set overall, we define a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Head of the Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *head* of a model is the part that is newly added to be specific to the new dataset. An *epoch* is one complete pass through the dataset. After calling `fit`, the results after each epoch are printed, showing the epoch number, the training and validation set losses (the \"measure of performance\" used for training the model), and any *metrics* you've requested (error rate, in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Epoch?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, when the model has seen every item in the training set, we call that an epoch. \n",
    "One complete pass through the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Loss?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A measure of how good the model is, chosen to drive training via SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Validation Set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of data held out from training, used only for measuring how good the model is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Hyperparameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in realistic scenarios we rarely build a model just by training its weight parameters once. Instead, we are likely to explore many versions of a model through various modeling choices regarding network architecture, learning rates, data augmentation strategies, and other factors. Many of these choices can be described as choices of *hyperparameters*. The word reflects that they are parameters about parameters, since they are the higher-level choices that govern the meaning of the weight parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Test Set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the automatic training process is in danger of overfitting the training data, we are in danger of overfitting the validation data through human trial and error and exploration.\n",
    "\n",
    "The solution to this conundrum is to introduce another level of even more highly reserved data, the *test set*. Just as we hold back the validation data from the training process, we must hold back the test set data even from ourselves. It cannot be used to improve the model; it can only be used to evaluate the model at the very end of our efforts. In effect, we define a hierarchy of cuts of our data, based on how fully we want to hide it from training and modeling processes: training data is fully exposed, the validation data is less exposed, and test data is totally hidden. This hierarchy parallels the different kinds of modeling and evaluation processes themselves—the automatic training process with back propagation, the more manual process of trying different hyper-parameters between training sessions, and the assessment of our final result.\n",
    "\n",
    "The test and validation sets should have enough data to ensure that you get a good estimate of your accuracy. If you're creating a cat detector, for instance, you generally want at least 30 cats in your validation set. That means that if you have a dataset with thousands of items, using the default 20% validation set size may be more than you need. On the other hand, if you have lots of data, using some of it for validation probably doesn't have any downsides.\n",
    "\n",
    "Having two levels of \"reserved data\"—a validation set and a test set, with one level representing data that you are virtually hiding from yourself—may seem a bit extreme. But the reason it is often necessary is because models tend to gravitate toward the simplest way to do good predictions (memorization), and we as fallible humans tend to gravitate toward fooling ourselves about how well our models are performing. The discipline of the test set helps us keep ourselves intellectually honest. That doesn't mean we *always* need a separate test set—if you have very little data, you may need to just have a validation set—but generally it's best to use one if at all possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Are Metrics?\n",
    "\n",
    "Metrics are **tools or numbers we use to measure how well an AI model is performing**. Think of them like a report card for your AI system—they tell you whether the model is doing a good job or needs improvement.\n",
    "\n",
    "In AI, metrics help us evaluate the quality of predictions made by the model compared to the actual correct answers (called \"ground truth\"). Different tasks (like classification, regression, or translation) use different metrics depending on what’s being measured.\n",
    "\n",
    "### Why Are Metrics Important?\n",
    "\n",
    "1. **To Evaluate Performance**: Metrics tell us if the model is accurate, fast, or reliable.\n",
    "2. **To Compare Models**: If you have two models, metrics help you decide which one works better.\n",
    "3. **To Improve Models**: Metrics highlight weaknesses, so you know what to fix during training.\n",
    "\n",
    "### Common Types of Metrics\n",
    "\n",
    "Here’s a breakdown of metrics used for different AI tasks:\n",
    "\n",
    "#### 1. **Classification Metrics**  \n",
    "   Used when the model predicts categories or labels (e.g., \"spam\" vs. \"not spam\").\n",
    "\n",
    "   - **Accuracy**:  \n",
    "     - Measures the percentage of correct predictions out of all predictions.\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "       $$\n",
    "     - Example: If a model gets 90 out of 100 predictions right, accuracy is 90%.\n",
    "\n",
    "   - **Precision**:  \n",
    "     - Measures how many of the predicted positives are actually correct.\n",
    "     - Useful when false positives are costly (e.g., flagging normal emails as spam).\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
    "       $$\n",
    "\n",
    "   - **Recall (Sensitivity)**:  \n",
    "     - Measures how many of the actual positives the model caught.\n",
    "     - Useful when missing positives is bad (e.g., detecting diseases).\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "       $$\n",
    "\n",
    "   - **F1-Score**:  \n",
    "     - A balance between precision and recall (useful when you want both to be good).\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "       $$\n",
    "\n",
    "#### 2. **Regression Metrics**  \n",
    "   Used when the model predicts continuous values (e.g., house prices, temperature).\n",
    "\n",
    "   - **Mean Absolute Error (MAE)**:  \n",
    "     - Measures the average difference between predicted and actual values.\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{MAE} = \\frac{\\sum |y_{\\text{predicted}} - y_{\\text{actual}}|}{n}\n",
    "       $$\n",
    "     - Example: If predictions are off by 2, 3, and 5 units, MAE = $(2 + 3 + 5) / 3 = 3.33$.\n",
    "\n",
    "   - **Mean Squared Error (MSE)**:  \n",
    "     - Similar to MAE but squares the differences, punishing larger errors more.\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{MSE} = \\frac{\\sum (y_{\\text{predicted}} - y_{\\text{actual}})^2}{n}\n",
    "       $$\n",
    "\n",
    "   - **Root Mean Squared Error (RMSE)**:  \n",
    "     - The square root of MSE, making it easier to interpret (same units as the data).\n",
    "     - Formula:  \n",
    "       $$\n",
    "       \\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "       $$\n",
    "\n",
    "#### 3. **Clustering Metrics**  \n",
    "   Used when grouping similar data points together (e.g., customer segmentation).\n",
    "\n",
    "   - **Silhouette Score**:  \n",
    "     - Measures how well each data point fits into its cluster compared to other clusters.\n",
    "     - Ranges from -1 (bad clustering) to +1 (good clustering).\n",
    "\n",
    "   - **Davies-Bouldin Index**:  \n",
    "     - Measures the average similarity between clusters (lower is better).\n",
    "\n",
    "#### 4. **Text Generation Metrics**  \n",
    "   Used for tasks like translation, summarization, or chatbots.\n",
    "\n",
    "   - **BLEU (Bilingual Evaluation Understudy)**:  \n",
    "     - Compares generated text to reference text, measuring word overlap.\n",
    "     - Higher scores mean closer matches.\n",
    "\n",
    "   - **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**:  \n",
    "     - Measures how much of the reference text is captured in the generated text.\n",
    "     - Commonly used for summarization.\n",
    "\n",
    "### Real-Life Analogy\n",
    "\n",
    "Think of metrics like checking the performance of a car:\n",
    "- **Accuracy**: How often the car reaches the destination without mistakes.\n",
    "- **Precision**: How close the car stays to the correct lane.\n",
    "- **Recall**: How well the car avoids missing turns.\n",
    "- **MAE/MSE**: How far the car deviates from the ideal speed or route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Pearson Correlation Coefficient (Person's r)?\n",
    "\n",
    "The **Pearson Correlation Coefficient** (often just called \"Pearson's r\") is a number that tells us how strongly two things are related to each other. It measures whether changes in one thing (like hours studied) are connected to changes in another thing (like exam scores). \n",
    "\n",
    "- The value of Pearson’s r ranges from **-1 to +1**:\n",
    "  - **+1**: Perfect positive relationship (as one goes up, the other goes up).\n",
    "  - **0**: No relationship (the two things don’t seem connected).\n",
    "  - **-1**: Perfect negative relationship (as one goes up, the other goes down).\n",
    "\n",
    "### How Does It Work? (Easy Explanation)\n",
    "\n",
    "Imagine plotting two variables on a graph:\n",
    "- One variable on the **x-axis** (e.g., hours studied).\n",
    "- The other on the **y-axis** (e.g., exam scores).\n",
    "\n",
    "If the points form a straight line sloping upward, Pearson’s r will be close to **+1**.  \n",
    "If the points form a straight line sloping downward, Pearson’s r will be close to **-1**.  \n",
    "If the points are scattered randomly, Pearson’s r will be close to **0**.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let’s say we want to see if there’s a relationship between **hours studied** and **exam scores** for 5 students:\n",
    "\n",
    "| Student | Hours Studied (X) | Exam Score (Y) |\n",
    "|---------|-------------------|----------------|\n",
    "| A       | 1                 | 50             |\n",
    "| B       | 2                 | 60             |\n",
    "| C       | 3                 | 70             |\n",
    "| D       | 4                 | 80             |\n",
    "| E       | 5                 | 90             |\n",
    "\n",
    "1. **Step 1: Plot the Data**\n",
    "   - On a graph, plot hours studied (x-axis) vs. exam scores (y-axis). You’ll see the points form a straight line sloping upward.\n",
    "\n",
    "2. **Step 2: Calculate Pearson’s r**\n",
    "   - Without going into complicated math, Pearson’s r here would be **+1**, because as hours studied increase, exam scores also increase perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Segmentation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a model that can recognize the content of every individual pixel in an image is called *segmentation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between Classifiction and Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification model is one which attempts to predict a class, or category. That is, it's predicting from a number of discrete possibilities, such as \"dog\" or \"cat.\" A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location. Sometimes people use the word _regression_ to refer to a particular kind of model called a _linear regression model_; this is a bad practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Data Augmentation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major challenge for object detection systems is that image labelling can be slow and expensive. There is a lot of work at the moment going into tools to try to make this labelling faster and easier, and to require fewer handcrafted labels to train accurate object detection models. One approach that is particularly helpful is to synthetically generate variations of input images, such as by rotating them or changing their brightness and contrast; this is called *data augmentation* and also works well for text and other types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Tabular Data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that is in the form of a table, such as from a spreadsheet, database, or CSV file. A tabular model is a model that tries to predict one column of a table based on information in other columns of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Confusion Matrix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a simple table that shows how well a classification model is performing by comparing its predictions to the actual results. It breaks down the predictions into four categories: correct predictions for both classes (true positives and true negatives) and incorrect predictions (false positives and false negatives). This helps you understand where the model is making mistakes, so you can improve it.\n",
    "\n",
    "The matrix displays the number of instances produced by the model on the test data.\n",
    "\n",
    "True Positive (TP): The model correctly predicted a positive outcome (the actual outcome was positive).\n",
    "True Negative (TN): The model correctly predicted a negative outcome (the actual outcome was negative).\n",
    "False Positive (FP): The model incorrectly predicted a positive outcome (the actual outcome was negative). Also known as a Type I error.\n",
    "False Negative (FN): The model incorrectly predicted a negative outcome (the actual outcome was positive). Also known as a Type II error.\n",
    "\n",
    "![Confusion Matrix](./images/confusion-Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Collaborative Filtering?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users.\n",
    "\n",
    "It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user. It looks at the items they like and combines them to create a ranked list of suggestions.\n",
    "\n",
    "Collaborative filtering solely uses past interactions between the customers and the products they’ve used to recommend new items. Item features are not important since user-item interactions are used and are stored in the user-item interactions matrix.\n",
    "\n",
    "In collaborative filtering, all the users are taken into consideration and people with similar tastes and preferences are used to suggest new and specific products to the primary customer. It helps companies and customers keep up with what’s trending.\n",
    "\n",
    "<img src=\"./images/2dMatrix.png\" width=700 height=500 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Backpropagation in Neural Network?\n",
    "Backpropagation is also known as \"Backward Propagation of Errors\" and it is a method used to train neural network . Its goal is to reduce the difference between the model’s predicted output and the actual output by adjusting the weights and biases in the network.\n",
    "\n",
    "In this article we will explore what backpropagation is, why it is crucial in machine learning and how it works.\n",
    "\n",
    "#### **What is Backpropagation?**\n",
    "Backpropagation is a technique used in deep learning to train artificial neural networks particularly feed-forward networks. It works iteratively to adjust weights and bias to minimize the cost function.\n",
    "\n",
    "In each epoch the model adapts these parameters reducing loss by following the error gradient. Backpropagation often uses optimization algorithms like gradient descent or stochastic gradient descent. The algorithm computes the gradient using the chain rule from calculus allowing it to effectively navigate complex layers in the neural network to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/backpropagation.png\" width=450 height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Backpropagation algorithm involves two main steps: the **Forward Pass** and the **Backward Pass**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Forward Pass**\n",
    "In forward pass the input data is fed into the input layer. These inputs combined with their respective weights are passed to hidden layers. For example in a network with two hidden layers (h1 and h2 as shown in Fig. (a)) the output from h1 serves as the input to h2. Before applying an activation function, a bias is added to the weighted inputs.\n",
    "\n",
    "Each hidden layer applies an activation function like ReLU (Rectified Linear Unit) which returns the input if it’s positive and zero otherwise. This adds non-linearity allowing the model to learn complex relationships in the data. Finally the outputs from the last hidden layer are passed to the output layer where an activation function such as softmax converts the weighted outputs into probabilities for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Backward Pass Work?**\n",
    "In the backward pass the error (the difference between the predicted and actual output) is propagated back through the network to adjust the weights and biases. One common method for error calculation is the Mean Squared Error (MSE) given by:\n",
    "\n",
    "$MSE=(Predicted Output−Actual Output)^2$\n",
    " \n",
    "\n",
    "Once the error is calculated the network adjusts weights using gradients which are computed with the chain rule. These gradients indicate how much each weight and bias should be adjusted to minimize the error in the next iteration. The backward pass continues layer by layer ensuring that the network learns and improves its performance. The activation function through its derivative plays a crucial role in computing these gradients during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"activation_function\"></a>\n",
    "# What is Activation Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **activation function** is a mathematical function applied to the output of a neuron in a neural network. It determines whether and to what extent a neuron should be activated (i.e., contribute to the next layer). Activation functions introduce **non-linearity**, enabling the network to learn complex patterns.\n",
    "\n",
    "### **How Many Are There?**\n",
    "There are several common activation functions, but the most widely used ones are:\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**:\n",
    "   - Formula: $ \\text{ReLU}(x) = \\max(0, x) $\n",
    "   - Output: $ 0 $ for negative inputs, $ x $ for positive inputs.\n",
    "   - Commonly used in hidden layers.\n",
    "\n",
    "2. **Sigmoid**:\n",
    "   - Formula: $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "   - Output: Between $ 0 $ and $ 1 $.\n",
    "   - Used in binary classification or output layers.\n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent)**:\n",
    "   - Formula: $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "   - Output: Between $ -1 $ and $ 1 $.\n",
    "   - Rarely used in hidden layers today.\n",
    "\n",
    "4. **Softmax**:\n",
    "   - Formula: $ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $\n",
    "   - Output: Probabilities that sum to $ 1 $.\n",
    "   - Used in multi-class classification output layers.\n",
    "\n",
    "5. **Leaky ReLU**:\n",
    "   - Formula: $ \\text{LeakyReLU}(x) = \\max(0.01x, x) $\n",
    "   - Output: Allows small negative values to avoid \"dying ReLU.\"\n",
    "\n",
    "### **Why Do We Need Activation Functions?**\n",
    "\n",
    "1. **Introduce Non-Linearity**:\n",
    "   - Without activation functions, a neural network would just perform a series of linear transformations, limiting it to solving only linear problems.\n",
    "\n",
    "2. **Enable Complex Learning**:\n",
    "   - Activation functions allow the network to approximate non-linear relationships, making it capable of solving real-world problems like image recognition, language processing, etc.\n",
    "\n",
    "3. **Control Output Range**:\n",
    "   - Some activation functions (e.g., sigmoid, softmax) constrain outputs to specific ranges, which is useful for tasks like classification.\n",
    "\n",
    "4. **Improve Training Stability**:\n",
    "   - Activation functions like ReLU help mitigate issues like the vanishing gradient problem, making training faster and more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ReLU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what_is_nlp\"></a>\n",
    "# What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP, or **Natural Language Processing**, is a part of artificial intelligence (AI) that helps computers understand, interpret, and respond to human language. It allows machines to read text, hear speech, and even generate responses in a way that feels natural to humans. For example, NLP powers virtual assistants like Siri or Alexa, chatbots, and tools that translate languages or summarize text. In short, it bridges the gap between human communication and machine understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tokenization\"></a>\n",
    "## What is Tokenization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into smaller, meaningful units called tokens . These tokens are the building blocks that help computers understand and process human language. Tokens can be words, subwords, characters, or even punctuation marks, depending on how you set up the tokenization process.\n",
    "\n",
    "Why is it Called **\"Tokenization\"**?\n",
    "The term \"token\" comes from the idea of using placeholders or symbols to represent pieces of data. In this case, each word, phrase, or character is treated as a \"token\"—a small, manageable unit of information. Think of tokens like Lego bricks: just as you build structures with Legos, we build meaning from text using tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example:**\n",
    "\n",
    "Sentence: \n",
    "```python\n",
    "\"I love AI!\"\n",
    "```\n",
    "\n",
    "Tokens: \n",
    "```python\n",
    "[\"I\", \"love\", \"AI\", \"!\"]\n",
    "```\n",
    "\n",
    "It’s like cutting a sentence into bite-sized chunks so computers can understand it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Types of Tokenization:**\n",
    "Word Tokenization : Split text into words.\n",
    "\n",
    "Example: `\"I love AI!\" → [\"I\", \"love\", \"AI\", \"!\"]`<br><br>\n",
    "\n",
    "Sentence Tokenization : Split text into sentences.\n",
    "\n",
    "Example: `\"I love AI. It’s amazing!\"` → `[\"I love AI.\", \"It’s amazing!\"]`<br><br>\n",
    "\n",
    "Subword Tokenization : Break words into smaller parts (used in advanced models like BERT).\n",
    "\n",
    "Example: `\"unhappiness\"` → `[\"un\", \"happy\", \"ness\"]`<br><br>\n",
    "\n",
    "Character Tokenization : Split text into individual characters.\n",
    "\n",
    "Example: `\"AI\"` → `[\"A\", \"I\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Feature Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of selecting, manipulating and transforming raw data into features that can be used in supervised learning. It consists of five processes: feature creation, transformations, feature extraction, exploratory data analysis and benchmarking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"long-tail_distribution\"></a>\n",
    "# What is Long Tail-Distribution?\n",
    "\n",
    "Some machine learning models struggle with datasets where a small number of values appear very frequently, while a large number of values appear rarely—this is known as a **long-tail distribution**.\n",
    "\n",
    "**For example:**\n",
    "\t•\tIn classification, if 95% of the data belongs to one class and only 5% to other classes, some models (like logistic regression or naive Bayes) may struggle because they tend to favor the majority class.\n",
    "\t•\tIn regression, if most data points are clustered around a small range, but a few have extremely large or small values (outliers), models like linear regression may not generalize well.\n",
    "\n",
    "Models that struggle with long-tail distributions:\n",
    "\t•\tLinear regression (sensitive to outliers)\n",
    "\t•\tLogistic regression (biased towards majority class)\n",
    "\t•\tDecision trees (can overfit to rare cases)\n",
    "\t•\tK-means clustering (centroids may not represent rare clusters well)\n",
    "\n",
    "Models that handle long-tail distributions better:\n",
    "\t•\tTree-based models (Random Forest, Gradient Boosting) can handle imbalanced data well.\n",
    "\t•\tNeural networks (if trained properly with techniques like weighted loss or data augmentation).\n",
    "\t•\tAnomaly detection methods (for rare events)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"numericalization\"></a>\n",
    "## What is Numericalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numericalization is the process of converting text (words, tokens, or characters) into numbers because **computers can only understand and process numbers**, not raw text. In NLP, this step is essential to prepare the data for machine learning or deep learning models.\n",
    "\n",
    "### How Does Numericalization Work?\n",
    "\n",
    "1. **Create a Vocabulary**:  \n",
    "   - First, we build a list of all unique words (or tokens) in the dataset. This list is called the **vocabulary**.\n",
    "   - Example: For the sentence `\"I love AI\"`, the vocabulary might look like this:\n",
    "     ```\n",
    "     Vocabulary: [\"I\", \"love\", \"AI\"]\n",
    "     ```\n",
    "\n",
    "2. **Assign Numbers to Words**:  \n",
    "   - Each word in the vocabulary is assigned a unique number (an index).\n",
    "   - Example:\n",
    "     ```\n",
    "     \"I\" → 1  \n",
    "     \"love\" → 2  \n",
    "     \"AI\" → 3\n",
    "     ```\n",
    "\n",
    "3. **Convert Tokens to Numbers**:  \n",
    "   - Replace each word in the text with its corresponding number from the vocabulary.\n",
    "   - Example: The sentence `\"I love AI\"` becomes:\n",
    "     ```\n",
    "     [1, 2, 3]\n",
    "     ```\n",
    "\n",
    "### Real-Life Analogy\n",
    "\n",
    "Think of numericalization like translating a foreign language into a language you understand:\n",
    "- Words are like foreign words (`\"I\"`, `\"love\"`, `\"AI\"`).\n",
    "- Numbers are like your native language (`1`, `2`, `3`).\n",
    "- You create a dictionary (vocabulary) to map foreign words to your language.\n",
    "\n",
    "### Types of Numericalization\n",
    "\n",
    "There are different ways to represent text numerically, depending on the task and model:\n",
    "\n",
    "#### 1. **Simple Indexing**\n",
    "   - Assign a unique number to each word.\n",
    "   - Example:\n",
    "     ```\n",
    "     Vocabulary: [\"I\", \"love\", \"AI\"]\n",
    "     Sentence: \"I love AI\" → [1, 2, 3]\n",
    "     ```\n",
    "\n",
    "#### 2. **One-Hot Encoding**\n",
    "   - Represent each word as a binary vector where only one element is `1` (indicating the word's position in the vocabulary), and the rest are `0`.\n",
    "   - Example:\n",
    "     ```\n",
    "     Vocabulary: [\"I\", \"love\", \"AI\"]\n",
    "     \"I\" → [1, 0, 0]  \n",
    "     \"love\" → [0, 1, 0]  \n",
    "     \"AI\" → [0, 0, 1]\n",
    "     ```\n",
    "\n",
    "#### 3. **Word Embeddings**\n",
    "   - Instead of using single numbers or one-hot vectors, words are represented as dense vectors (lists of numbers) that capture their meaning.\n",
    "   - Example:\n",
    "     ```\n",
    "     \"I\" → [0.1, 0.5, -0.3]  \n",
    "     \"love\" → [0.7, -0.2, 0.4]  \n",
    "     \"AI\" → [-0.6, 0.8, 0.1]\n",
    "     ```\n",
    "   - These embeddings are learned during training and are more powerful than simple indexing or one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Universal Language Model Fine-tuning (ULMFit)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ULMFit, short for Universal Language Model Fine-tuning, is a revolutionary approach in natural language processing (NLP), a field of artificial intelligence (AI) that focuses on the interaction between computers and human language. This method, developed by fast.ai, is significant because it was one of the first to show that a pre-trained language model could be adapted effectively to various NLP tasks, improving performance dramatically.\n",
    "\n",
    "In simple terms, ULMFit involves training a language model on a large body of the text first. This initial step allows the model to learn the general structure of a language like English, for instance, and understand how words and phrases typically come together. It’s a bit like how a child learns a language by listening to conversations around them, picking up patterns and meanings over time. Once this base knowledge is established, ULMFit then applies this understanding to more specific tasks, such as text classification, sentiment analysis, or question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
