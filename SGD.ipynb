{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import torch\n",
    "import numpy\n",
    "import graphviz\n",
    "import utilities\n",
    "\n",
    "def gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"' + s + '; }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_sample.tgz already exists. Skipping download.\n",
      "./mnist_sample already exists. Skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL and the local file path where the file will be saved\n",
    "# Set the base path\n",
    "file_name = \"mnist_sample.tgz\"\n",
    "extract_path = \"./mnist_sample\"\n",
    "\n",
    "path = utilities.untar_data(\n",
    "  'https://s3.amazonaws.com/fast-ai-sample/mnist_sample.tgz', \n",
    "  file_name, \n",
    "  extract_path\n",
    "  )\n",
    "path = (path/'mnist_sample')\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = sorted((path/'train'/'3').iterdir())\n",
    "sevens = sorted((path/'train'/'7').iterdir())\n",
    "\n",
    "seven_tensors = [torch.tensor(numpy.array(Image.open(o))) for o in sevens]\n",
    "three_tensors = [torch.tensor(numpy.array(Image.open(o))) for o in threes]\n",
    "\n",
    "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
    "stacked_threes = torch.stack(three_tensors).float()/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is a simple yet powerful optimization algorithm used to train machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TLDR**; <br>\n",
    " In summary, following is the process of SGD.\n",
    "\n",
    "1. **Stochastic**: Pick one data point (or a small batch) randomly from the dataset.\n",
    "2. **Gradient**: Compute the gradient of the loss with respect to each parameter (weights and biases) for that data point.\n",
    "3. **Learning Rate**: Multiply the gradient by the learning rate ($ \\eta $) to control the step size.\n",
    "4. **Update Parameters**: Adjust the parameters using:\n",
    "   $$\n",
    "   \\text{New Parameter} = \\text{Old Parameter} - \\eta \\cdot \\text{Gradient}.\n",
    "   $$\n",
    "5. **Repeat**: Go back to Step 1 and repeat the process for the next data point (or mini-batch).\n",
    "6. **Convergence**: Continue iterating until the loss stops decreasing significantly or a set number of iterations is reached.\n",
    "\n",
    "In short: **Stochastic → Gradient → Learning Rate → Update → Repeat → Converge**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Easy to understand example:**<br><br>\n",
    "You’re standing on a hilly landscape, and your goal is to find the lowest point (the valley). Each step you take moves you closer to the bottom.\n",
    "- The gradient tells you which direction is downhill.\n",
    "- The learning rate controls how big each step is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will learn each of these steps in more details. Just if you don't understand what's happening, come back and read the TLDR again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the way that Arthur Samuel described machine learning, which we quoted in <<chapter_intro>>?\n",
    "\n",
    "> : Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would \"learn\" from its experience.\n",
    "\n",
    "As we discussed, this is the key to allowing us to have a model that can get better and better—that can learn. But our pixel similarity approach does not really do this. We do not have any kind of weight assignment, or any way of improving based on testing the effectiveness of a weight assignment. In other words, we can't really improve our pixel similarity approach by modifying a set of parameters. In order to take advantage of the power of deep learning, we will first have to represent our task in the way that Arthur Samuel described it.\n",
    "\n",
    "Instead of trying to find the similarity between an image and an \"ideal image,\" we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category—for instance the probability of being the number 8:\n",
    "\n",
    "```\n",
    "def pr_eight(x,w): return (x*w).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are assuming that `x` is the image, represented as a vector—in other words, with all of the rows stacked up end to end into a single long line. And we are assuming that the weights are a vector `w`. If we have this function, then we just need some way to update the weights to make them a little bit better. With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them.\n",
    "\n",
    "We want to find the specific values for the vector `w` that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not. Searching for the best vector `w` is a way to search for the best function for recognising 8s. (Because we are not yet using a deep neural network, we are limited by what our function can actually do—we are going to fix that constraint later in this chapter.) \n",
    "\n",
    "To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier:\n",
    "\n",
    "1. *Initialize* the weights.\n",
    "1. For each image, use these weights to *predict* whether it appears to be a 3 or a 7.\n",
    "1. Based on these predictions, calculate how good the model is (its *loss*).\n",
    "1. Calculate the *gradient*, which measures for each weight, how changing that weight would change the loss\n",
    "1. *Step* (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and *repeat* the process.\n",
    "1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seven steps, illustrated in <<gradient_descent>>, are the key to the training of all deep learning models. That deep learning turns out to rely entirely on these steps is extremely surprising and counterintuitive. It's amazing that this process can solve such complex problems. But, as you'll see, it really does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"596pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 596.25 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-74 592.25,-74 592.25,4 -4,4\"/>\n",
       "<!-- init -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>init</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">init</text>\n",
       "</g>\n",
       "<!-- predict -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>predict</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"127.51\" cy=\"-18\" rx=\"36.51\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.51\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">predict</text>\n",
       "</g>\n",
       "<!-- init&#45;&gt;predict -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>init&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.4,-18C62.17,-18 70.91,-18 79.55,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.26,-21.5 89.26,-18 79.26,-14.5 79.26,-21.5\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"228.02\" cy=\"-52\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"228.02\" y=\"-46.95\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predict&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>predict&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.06,-28.2C168.95,-31.96 181.36,-36.24 192.63,-40.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.4,-43.41 201.99,-43.36 193.68,-36.79 191.4,-43.41\"/>\n",
       "</g>\n",
       "<!-- gradient -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>gradient</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"365.13\" cy=\"-52\" rx=\"41.12\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"365.13\" y=\"-46.95\" font-family=\"Times,serif\" font-size=\"14.00\">gradient</text>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;gradient -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>loss&#45;&gt;gradient</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M255.39,-52C271.56,-52 292.81,-52 312.24,-52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.21,-55.5 322.21,-52 312.21,-48.5 312.21,-55.5\"/>\n",
       "</g>\n",
       "<!-- step -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>step</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"470.25\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"470.25\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">step</text>\n",
       "</g>\n",
       "<!-- gradient&#45;&gt;step -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>gradient&#45;&gt;step</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M398.76,-41.24C410.3,-37.44 423.3,-33.15 434.97,-29.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"435.82,-32.71 444.22,-26.25 433.63,-26.06 435.82,-32.71\"/>\n",
       "</g>\n",
       "<!-- step&#45;&gt;predict -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>step&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M443.04,-18C385.63,-18 248.13,-18 175.42,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"175.84,-14.5 165.84,-18 175.84,-21.5 175.84,-14.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"289.52\" y=\"-21.2\" font-family=\"Times,serif\" font-size=\"14.00\">repeat</text>\n",
       "</g>\n",
       "<!-- stop -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"561.25\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"561.25\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">stop</text>\n",
       "</g>\n",
       "<!-- step&#45;&gt;stop -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>step&#45;&gt;stop</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M497.47,-18C505.37,-18 514.19,-18 522.67,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"522.39,-21.5 532.39,-18 522.39,-14.5 522.39,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x338ece3d0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id gradient_descent\n",
    "#caption The gradient descent process\n",
    "#alt Graph showing the steps for Gradient Descent\n",
    "gv('''\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to do each of these seven steps.\n",
    "These are the details that make a big difference for deep learning practitioners, but it turns out that the general approach to each one generally follows some basic principles. Here are a few guidelines:\n",
    "\n",
    "- Initialize:: We initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category—but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.\n",
    "- Loss:: This is what Samuel referred to when he spoke of *testing the effectiveness of any current weight assignment in terms of actual performance*. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).\n",
    "- Step:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating *gradients*. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.\n",
    "- Stop:: Once we've decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one magic step is the bit where we calculate the gradients. As we mentioned, we use calculus as a performance optimization; it allows us to more quickly calculate whether our loss will go up or down when we adjust our parameters up or down. In other words, the gradients will tell us how much we have to change each weight to make our model better.\n",
    "\n",
    "You may remember from your high school calculus class that the *derivative* of a function tells you how much a change in its parameters will change its result. If not, don't worry, lots of us forget calculus once high school is behind us! But you will have to have some intuitive understanding of what a derivative is before you continue, so if this is all very fuzzy in your head, head over to Khan Academy and complete the [lessons on basic derivatives](https://www.khanacademy.org/math/differential-calculus/dc-diff-intro). You won't have to know how to calculate them yourselves, you just have to know what a derivative is.\n",
    "\n",
    "The key point about a derivative is this: for any function, such as the quadratic function we saw in the previous section, we can calculate its derivative. The derivative is another function. It calculates the change, rather than the value. For instance, the derivative of the quadratic function at the value 3 tells us how rapidly the function changes at the value 3. More specifically, you may recall that gradient is defined as *rise/run*, that is, the change in the value of the function, divided by the change in the value of the parameter. When we know how our function will change, then we know what we need to do to make it smaller. This is the key to machine learning: having a way to change the parameters of a function to make it smaller. Calculus provides us with a computational shortcut, the derivative, which lets us directly calculate the gradients of our functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping With a Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding how to change our parameters based on the values of the gradients is an important part of the deep learning process. Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the **learning rate** (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything. Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we'll show you a better approach later in this book, called the **learning rate finder**). Once you've picked a learning rate, you can adjust your parameters using this simple function:\n",
    "\n",
    "```\n",
    "w -= gradient(w) * lr\n",
    "```\n",
    "\n",
    "This is known as **stepping** your parameters, using an **optimizer step**. Notice how we _subtract_ the `gradient * lr` from the parameter to update it.  This allows us to adjust the parameter in the direction of the [slope](math.ipynb/#algebra-slope) by increasing the parameter when the slope is negative and decreasing the parameter when the slope is positive.  We want to adjust our parameters in the direction of the slope because our goal in deep learning is to _minimize_ the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to look at an SGD example and see how finding a minimum can be used to train a model to fit data better.\n",
    "\n",
    "Let's start with a simple, synthetic, example model. Imagine you were measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, and then get slower as it went up the hill; it would be slowest at the top, and it would then speed up again as it went downhill. You want to build a model of how the speed changes over time. If you were measuring the speed manually every second for 20 seconds, it might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(189., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x): return (x**2).sum()\n",
    "\n",
    "xt = torch.tensor([3., 4., 8., 10.]).requires_grad_()\n",
    "f(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19.])\n",
      "Speed: tensor([66.3066, 54.2247, 42.2210, 34.2710, 20.5757, 16.5216, 13.8627,  4.8435,\n",
      "        -2.5848,  3.6991, -1.6452,  2.8018,  5.9687,  4.1017, 18.6041, 27.4000,\n",
      "        30.5408, 44.0104, 54.6240, 66.6288])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x155514210>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time = torch.arange(0,20).float()\n",
    "print(f\"Time: {time}\")\n",
    "speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1\n",
    "print(f\"Speed: {speed}\")\n",
    "\n",
    "plt.scatter(time, speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to distinguish clearly between the function's input (the time when we are measuring the coaster's speed) \n",
    "# and its parameters (the values that define *which* quadratic we're trying).\n",
    "def f(t, params):\n",
    "    a,b,c = params\n",
    "    return a*(t**2) + (b*t) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Initialize a random number\n",
    "params = torch.randn(3).requires_grad_()\n",
    "orig_params = params.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([  0.7820,   2.1110,   3.9940,   6.4309,   9.4218,  12.9667,  17.0656,\n",
      "         21.7185,  26.9253,  32.6861,  39.0009,  45.8697,  53.2925,  61.2692,\n",
      "         69.7999,  78.8846,  88.5233,  98.7159, 109.4625, 120.7631],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step2: Calculate the predictions\n",
    "preds = f(time, params)\n",
    "print(f\"Predictions: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_preds(preds, ax=None):\n",
    "    if ax is None: ax=plt.subplots()[1]\n",
    "    numpy_array = preds.detach().numpy()\n",
    "    ax.scatter(time, speed)\n",
    "    ax.scatter(time, numpy_array, color='red')\n",
    "    ax.set_ylim(-300, 100)\n",
    "\n",
    "show_preds(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "def mse(predictions, actuals): return ((predictions - actuals)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 1898.379638671875\n"
     ]
    }
   ],
   "source": [
    "#Step3: Measure the loss\n",
    "loss = mse(preds, speed)\n",
    "print(f\"MSE Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: tensor([11978.9609,   791.3469,    39.2707])\n",
      "Updated Parameters: tensor([0.2770, 1.0520, 0.7820], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the gradient\n",
    "loss.backward()\n",
    "\n",
    "# Step 4: Pick a leaning rate, For now just a small number later we will learn how to pick the right learning rate\n",
    "params.grad * 1e-5\n",
    "print(f\"Gradient: {params.grad}\")\n",
    "print(f\"Updated Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the parameters based on the calculated gradient\n",
    "lr = 1e-5\n",
    "params.data -= lr * params.grad.data\n",
    "params.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 864.2888793945312\n"
     ]
    }
   ],
   "source": [
    "# Let's see if the loss improved\n",
    "preds = f(time,params)\n",
    "print(f\"MSE Loss: {mse(preds, speed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the process till the loss is minimized\n",
    "def apply_step(params, prn=True):\n",
    "    preds = f(time, params)\n",
    "    loss = mse(preds, speed)\n",
    "    loss.backward()\n",
    "    params.data -= lr * params.grad.data\n",
    "    params.grad = None\n",
    "    if prn: print(loss.item())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864.2888793945312\n",
      "668.6015014648438\n",
      "631.5659790039062\n",
      "624.5521240234375\n",
      "623.2192993164062\n",
      "622.9615478515625\n",
      "622.9072265625\n",
      "622.8914184570312\n",
      "622.8828735351562\n",
      "622.8756713867188\n"
     ]
    }
   ],
   "source": [
    "# Iterate by looping many improvements\n",
    "for i in range(10): apply_step(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we plot the function at every step. \n",
    "# Then we can see how the shape is approaching the best possible quadratic function for our data:\n",
    "_,axs = plt.subplots(1,4,figsize=(12,3))\n",
    "for ax in axs: show_preds(apply_step(params, False), ax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, at the beginning, the weights of our model can be random (training *from scratch*) or come from a pretrained model (*transfer learning*). In the first case, the output we will get from our inputs won't have anything to do with what we want, and even in the second case, it's very likely the pretrained model won't be very good at the specific task we are targeting. So the model will need to *learn* better weights.\n",
    "\n",
    "We begin by comparing the outputs the model gives us with our targets (we have labeled data, so we know what result the model should give) using a *loss function*, which returns a number that we want to make as low as possible by improving our weights. To do this, we take a few data items (such as images) from the training set and feed them to our model. We compare the corresponding targets using our loss function, and the score we get tells us how wrong our predictions were. We then change the weights a little bit to make it slightly better.\n",
    "\n",
    "To find how to change the weights to make the loss a bit better, we use calculus to calculate the *gradients*. (Actually, we let PyTorch do it for us!) Let's consider an analogy. Imagine you are lost in the mountains with your car parked at the lowest point. To find your way back to it, you might wander in a random direction, but that probably wouldn't help much. Since you know your vehicle is at the lowest point, you would be better off going downhill. By always taking a step in the direction of the steepest downward slope, you should eventually arrive at your destination. We use the magnitude of the gradient (i.e., the steepness of the slope) to tell us how big a step to take; specifically, we multiply the gradient by a number we choose called the *learning rate* to decide on the step size. We then *iterate* until we have reached the lowest point, which will be our parking lot, then we can *stop*.\n",
    "\n",
    "All of that we just saw can be transposed directly to the MNIST dataset, except for the loss function. Let's now see how we can define a good training objective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are We Trying to Achieve?\n",
    "\n",
    "We are preparing data for training a machine learning model. Specifically:\n",
    "1. **Input Data (`train_x`)**: These are the images of handwritten digits (3s and 7s). We need to convert these images into a format the model can understand.\n",
    "2. **Labels (`train_y`)**: These are the \"answers\" or categories for each image. For example, we label all images of `3` as `1` and all images of `7` as `0`.\n",
    "\n",
    "Once the data is prepared, we can use it to train a model that learns to distinguish between `3`s and `7`s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"preparing_data\"></a>\n",
    "### Step 1: Preparing the Input Data (`train_x`)\n",
    "\n",
    "#### What Are the Images?\n",
    "- Each image is a grid of pixels (e.g., 28x28 pixels for MNIST digits).\n",
    "- In PyTorch, these images are stored as tensors (multi-dimensional arrays).\n",
    "\n",
    "#### Why Change the Shape?\n",
    "- A single image is a **matrix** (rank-2 tensor) of size `(28, 28)` (height x width).\n",
    "- To feed these images into most machine learning models, we need to \"flatten\" them into **vectors** (rank-1 tensors) of size `(784,)` (since $ 28 \\times 28 = 784 $).\n",
    "\n",
    "#### How Do We Flatten Them?\n",
    "- We use the `.view(-1, 28*28)` method:\n",
    "  - `-1`: Automatically calculates how many rows are needed based on the data.\n",
    "  - `28*28`: Flattens each image into a vector of length 784.\n",
    "\n",
    "#### Combining All Images\n",
    "- We have two groups of images: `stacked_threes` (all images of `3`) and `stacked_sevens` (all images of `7`).\n",
    "- We combine them into one big tensor using `torch.cat()`.\n",
    "\n",
    "#### Final Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1647, 0.4627, 0.8588,\n",
       "         0.6510, 0.4627, 0.4627, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4039, 0.9490, 0.9961,\n",
       "         0.9961, 0.9961, 0.9961, 0.9961, 0.2588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.9098,\n",
       "         0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9333, 0.2745, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.4078, 0.9569, 0.9961, 0.8784, 0.9961, 0.9961, 0.9961, 0.5529, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.8118, 0.9961, 0.8235, 0.9961, 0.9961, 0.9961, 0.1333,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.3294, 0.8078, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "         0.1608, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0941, 0.8196, 0.9961, 0.9961,\n",
       "         0.9961, 0.6706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3569, 0.5373, 0.9922, 0.9961,\n",
       "         0.9961, 0.9961, 0.4392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1569, 0.8392, 0.9804, 0.9961, 0.9961,\n",
       "         0.9961, 0.9961, 0.9961, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9686, 0.9961, 0.9961,\n",
       "         0.9961, 0.9961, 0.9961, 0.9961, 0.5725, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4314, 0.9647,\n",
       "         0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.6706, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2863, 0.3490, 0.3490, 0.3647, 0.9412, 0.9961, 0.6706, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.5020, 0.9961, 0.8588, 0.1216,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275, 0.9961, 0.9961, 0.8392,\n",
       "         0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5412, 0.9961, 0.9961,\n",
       "         0.4549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.6941,\n",
       "         0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.9412, 0.9961,\n",
       "         0.9961, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6431,\n",
       "         0.9961, 0.8431, 0.2471, 0.1412, 0.0000, 0.2000, 0.3490, 0.8078, 0.9961,\n",
       "         0.9961, 0.5451, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2235, 0.7725, 0.9961, 0.9961, 0.8706, 0.7059, 0.9451, 0.9961, 0.9961,\n",
       "         0.9922, 0.8353, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.5490, 0.4118, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "         0.9961, 0.9255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0275, 0.4588, 0.4588, 0.6471, 0.9961,\n",
       "         0.9961, 0.9373, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
    "train_x[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This creates a single tensor where:\n",
    "  - Each **row** represents one flattened image (a vector of 784 pixels).\n",
    "  - The total number of rows is the total number of images (`len(threes) + len(sevens)`).\n",
    "\n",
    "### Step 2: Creating Labels (`train_y`)\n",
    "\n",
    "#### What Are Labels?\n",
    "- Labels tell the model what each image represents:\n",
    "  - `1` means the image is a `3`.\n",
    "  - `0` means the image is a `7`.\n",
    "\n",
    "#### How Do We Create Labels?\n",
    "- For all `3`s, we create a list of `1`s with the same length as the number of `3`s.\n",
    "- For all `7`s, we create a list of `0`s with the same length as the number of `7`s.\n",
    "- We concatenate these lists into one.\n",
    "\n",
    "#### Adding an Extra Dimension\n",
    "- Machine learning models often expect labels to be in a specific shape, like a column vector (rank-2 tensor with shape `(n, 1)`).\n",
    "- We use `.unsqueeze(1)` to add an extra dimension to the labels tensor.\n",
    "\n",
    "#### Final Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = torch.tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- This creates a tensor where:\n",
    "  - Each row corresponds to the label of one image.\n",
    "  - The shape is `(n, 1)`, where `n` is the total number of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preparing_data_step_3\"></a>\n",
    "### Step 3: Prepare the data for PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Dataset](optional_read.ipynb/#pytorch-dataset) in PyTorch is required to return a tuple of `(x,y)` when indexed. Python provides a `zip` function which, when combined with `list`, provides a simple way to get this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(train_x,train_y))\n",
    "x,y = dataset[0]\n",
    "\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we need to prepare our validation sets the same way as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_3_files = (path/'valid'/'3').iterdir()\n",
    "valid_3_tens = torch.stack([torch.tensor(numpy.array(Image.open(o))) for o in valid_3_files])\n",
    "valid_3_tens = valid_3_tens.float() / 255\n",
    "\n",
    "# Load and process validation images for '7'\n",
    "valid_7_files = (path/'valid'/'7').iterdir()\n",
    "valid_7_tens = torch.stack([torch.tensor(numpy.array(Image.open(o))) for o in valid_7_files])\n",
    "valid_7_tens = valid_7_tens.float() / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
    "valid_y = torch.tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
    "valid_dset = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is now prepared for training, let's dive into the actual training steps.\n",
    "As you may recall: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"596pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 596.25 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-74 592.25,-74 592.25,4 -4,4\"/>\n",
       "<!-- init -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>init</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">init</text>\n",
       "</g>\n",
       "<!-- predict -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>predict</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"127.51\" cy=\"-18\" rx=\"36.51\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127.51\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">predict</text>\n",
       "</g>\n",
       "<!-- init&#45;&gt;predict -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>init&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.4,-18C62.17,-18 70.91,-18 79.55,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.26,-21.5 89.26,-18 79.26,-14.5 79.26,-21.5\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"228.02\" cy=\"-52\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"228.02\" y=\"-46.95\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predict&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>predict&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.06,-28.2C168.95,-31.96 181.36,-36.24 192.63,-40.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.4,-43.41 201.99,-43.36 193.68,-36.79 191.4,-43.41\"/>\n",
       "</g>\n",
       "<!-- gradient -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>gradient</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"365.13\" cy=\"-52\" rx=\"41.12\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"365.13\" y=\"-46.95\" font-family=\"Times,serif\" font-size=\"14.00\">gradient</text>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;gradient -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>loss&#45;&gt;gradient</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M255.39,-52C271.56,-52 292.81,-52 312.24,-52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.21,-55.5 322.21,-52 312.21,-48.5 312.21,-55.5\"/>\n",
       "</g>\n",
       "<!-- step -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>step</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"470.25\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"470.25\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">step</text>\n",
       "</g>\n",
       "<!-- gradient&#45;&gt;step -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>gradient&#45;&gt;step</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M398.76,-41.24C410.3,-37.44 423.3,-33.15 434.97,-29.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"435.82,-32.71 444.22,-26.25 433.63,-26.06 435.82,-32.71\"/>\n",
       "</g>\n",
       "<!-- step&#45;&gt;predict -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>step&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M443.04,-18C385.63,-18 248.13,-18 175.42,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"175.84,-14.5 165.84,-18 175.84,-21.5 175.84,-14.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"289.52\" y=\"-21.2\" font-family=\"Times,serif\" font-size=\"14.00\">repeat</text>\n",
       "</g>\n",
       "<!-- stop -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"561.25\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"561.25\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">stop</text>\n",
       "</g>\n",
       "<!-- step&#45;&gt;stop -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>step&#45;&gt;stop</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M497.47,-18C505.37,-18 514.19,-18 522.67,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"522.39,-21.5 532.39,-18 522.39,-14.5 522.39,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x1554ae3d0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id gradient_descent\n",
    "#caption The gradient descent process\n",
    "#alt Graph showing the steps for Gradient Descent\n",
    "gv('''\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using **Logistic Regression** (a classification model). The purpose is to train the model to distinguish between two classes (3s and 7s) by learning a decision boundary using weights and bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear model, the prediction is calculated as: $y=w⋅x+b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, the `w` in the equation $y=w*x+b$ is called the *weights*, and the `b` is called the *bias*. Together, the weights and bias make up the *parameters*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need an (initially random) weight for every pixel (this is the *initialize* step in our seven-step process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random weights for a model and prepares them for training:\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We’re creating initial weights for a model, scaled appropriately, and ensuring they’re ready for gradient-based optimization.<br><br> 1. **`torch.randn(size)`**: Creates random numbers from a normal distribution (mean = 0, std = 1) with the given shape (`size`).<br>2. **`* std`**: Scales the random numbers to control their range (e.g., smaller `std` makes weights closer to 0).<br>3. **`.requires_grad_()`**: Tells PyTorch to track these weights so their gradients can be calculated during training.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**What Is std?** <br>\n",
    "In short, std is to adjust the initial scale of the weights by making them larger or smaller at the start of training. This helps ensure the weights are in a good range for effective learning, but it doesn’t directly depend on the loss function.<br><br>\n",
    "std stands for standard deviation , which controls the spread or scale of the random numbers. <br><br>\n",
    "By multiplying the random numbers by std, we adjust their range. For example: <br><br>\n",
    "If std = 1.0, the random numbers stay as-is (standard normal distribution). <br>\n",
    "If std = 0.01, the random numbers are scaled down to be much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create weights with size (28*28, 1) because:\n",
    "- Each image is flattened into a vector of 784 pixels (features),\n",
    "- We need one weight for each pixel to compute a weighted sum,\n",
    "- The model produces a single output (e.g., a prediction for binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4702],\n",
       "        [ 2.1746],\n",
       "        [-0.3294],\n",
       "        [-1.3069],\n",
       "        [-1.7004]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = init_params((28*28,1))\n",
    "weights[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `weights*pixels` won't be flexible enough—it is always equal to 0 when the pixels are equal to 0 (i.e., its *intercept* is 0). You might remember from high school math that the formula for a line is `y=w*x+b`; we still need the `b`. We'll initialize it to a random number too:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bias adds flexibility to the model by allowing it to shift predictions up or down, ensuring it can fit data even when inputs are zero. Like weights, the bias is updated during training to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5504], requires_grad=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = init_params(1)\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate a prediction for one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.5449], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_image = train_x[0]\n",
    "weights_transpose = weights.T\n",
    "\n",
    "one_image_predication = (one_image * weights_transpose).sum() + bias\n",
    "one_image_predication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `T` in `weights.T` stands for **transpose**. It’s a mathematical operation that flips the dimensions of a tensor or matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can read more details about `.T` and how it works in Optional read notebook [.T Pytorch Transpose In Details](optional_read.ipynb#pytorch-transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you are having hard time understanding what just happened here, read the simpler illustration in Optional reads Notebook [SGD Illustrate with code](optional_read.ipynb#sgd-illustrate-with-code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above example was just for one image in our dataset, now how do we do this for all the images to get prediction score? You might be thinking, we will loop through all the dataset and apply the function.\n",
    "We could! However, that would be very slow. Because Python loops don't run on the GPU, and because Python is a slow language for loops in general, we need to represent as much of the computation in a model as possible using higher-level functions.\n",
    "\n",
    "In this case, there's an extremely convenient mathematical operation that calculates `w*x` for every row of a matrix—it's called *matrix multiplication*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, matrix multiplication is represented with the `@` operator. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14.5449],\n",
       "        [11.3809],\n",
       "        [15.4577],\n",
       "        ...,\n",
       "        [ 1.7349],\n",
       "        [16.5985],\n",
       "        [ 5.3557]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear1(xb): return xb@weights + bias\n",
    "preds = linear1(train_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> More details about this operation which is a __Linear Transformation and Matrix Multiplication__,  can be find [Linear Transformation: Matrix Multiplication](optional_read.ipynb#linear-transformation-matrix-multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element is the same as we calculated before, as we'd expect. This equation, `batch@weights + bias`, is one of the two fundamental equations of any neural network (the other one is the *activation function*, which we'll see in a moment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our accuracy. To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0.0, so our accuracy for each item can be calculated (using broadcasting, so no loops!) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects = (preds>0.0).float() == train_y\n",
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5209745168685913"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects.float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `.item()` extracts the value from the tensor as a plain Python number (e.g., `0.75` instead of `tensor(0.75)`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the change in accuracy is for a small change in one of the weights (note that we have to ask PyTorch not to calculate gradients as we do this, which is what `with torch.no_grad()` is doing here):\n",
    "If you try to modify `weights[0]` without `torch.no_grad()`, PyTorch will raise an error because it tries to track the operation for gradient computation\n",
    "When gradient tracking is disabled, any operations performed inside the with `torch.no_grad():` block will not:\n",
    "- Compute gradients,\n",
    "- Update the computation graph (which tracks how outputs depend on inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code updates one of the weights in the model without tracking gradients.\n",
    "with torch.no_grad(): weights[0] *= 1.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `with` syntax is same as following:\n",
    ">```python\n",
    ">try:\n",
    "    >torch.no_grad().__enter__()  # Disable gradient tracking\n",
    "   >weights[0] *= 1.0001         # Slightly increase the first weight\n",
    ">except Exception as e:\n",
    "    >print(f\"An error occurred: {e}\")\n",
    ">finally:\n",
    "    >torch.no_grad().__exit__(None, None, None)  # Re-enable gradient tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5209745168685913"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = linear1(train_x)\n",
    "((preds>0.0).float() == train_y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates that:\n",
    "- A single small tweak to one weight isn’t enough to significantly improve the model.\n",
    "- To train the model effectively, we need to update all the weights simultaneously using gradients calculated from the loss function.\n",
    "- This is why we rely on Stochastic Gradient Descent (SGD) or similar optimization algorithms—they compute gradients for all weights and adjust them in a coordinated way to minimize the loss.\n",
    "\n",
    "As we've seen, we need gradients in order to improve our model using SGD, and in order to calculate gradients we need some *loss function* that represents how good our model is. That is because the gradients are a measure of how that loss function changes with small tweaks to the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function—if we do, most of the time our gradients will actually be 0, and the model will not be able to learn from that number.\n",
    "\n",
    "> S: In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5), so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are 0 or infinite, which are useless for updating the model.\n",
    "\n",
    "Instead, we need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss. So what does a \"slightly better prediction\" look like, exactly? Well, in this case, it means that if the correct answer is a 3 the score is a little higher, or if the correct answer is a 7 the score is a little lower.\n",
    "\n",
    "Let's write such a function now. What form does it take?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The loss function receives not the images themselves, but the predictions from the model. Let's make one argument, `prds`, of values between 0 and 1, where each value is the prediction that an image is a 3. It is a vector (i.e., a rank-1 tensor), indexed over the images.\n",
    "\n",
    "The purpose of the loss function is to measure the difference between predicted values and the true values — that is, the targets (aka labels). Let's make another argument, `trgts`, with values of 0 or 1 which tells whether an image actually is a 3 or not. It is also a vector (i.e., another rank-1 tensor), indexed over the images.\n",
    "\n",
    "So, for instance, suppose we had three images which we knew were a 3, a 7, and a 3. And suppose our model predicted with high confidence (`0.9`) that the first was a 3, with slight confidence (`0.4`) that the second was a 7, and with fair confidence (`0.2`), but incorrectly, that the last was a 7. This would mean our loss function would receive these values as its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgts  = torch.tensor([1,0,1])\n",
    "prds   = torch.tensor([0.9, 0.4, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a first try at a loss function that measures the distance between `predictions` and `targets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a new function, `torch.where(a,b,c)`. This is the same as running the list comprehension `[b[i] if a[i] else c[i] for i in range(len(a))]`, except it works on tensors, at C/CUDA speed. In plain English, this function will measure how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0, and then it will take the mean of all those distances.\n",
    "\n",
    "> note: Read the Docs: It's important to learn about PyTorch functions like this, because looping over tensors in Python performs at Python speed, not C/CUDA speed! Try running `help(torch.where)` now to read the docs for this function, or, better still, look it up on the PyTorch documentation site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__Why Use `1 - prds`?__<br>\n",
    ">When the true label is 1 (the image is a 3), we want the model to predict a value close to 1.\n",
    ">If the prediction (prds) is far from 1, the loss should be high.\n",
    ">\n",
    ">Suppose the true label is 1 and the predictions are:\n",
    ">- Prediction = `0.9`: `1 - 0.9 = 0.1` (small penalty, good prediction).\n",
    ">- Prediction = `0.2`: `1 - 0.2 = 0.8` (large penalty, bad prediction).\n",
    ">\n",
    ">This ensures that predictions closer to 1 are rewarded, while predictions far from 1 are penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on our `prds` and `trgts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.4000, 0.8000])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(trgts==1, 1-prds, prds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this function returns a lower number when predictions are more accurate, when accurate predictions are more confident (higher absolute values), and when inaccurate predictions are less confident. In PyTorch, we always assume that a lower value of a loss function is better. Since we need a scalar for the final loss, `mnist_loss` takes the mean of the previous tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4333)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(prds,trgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if we change our prediction for the one \"false\" target from `0.2` to `0.8` the loss will go down, indicating that this is a better prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2333)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_loss(torch.tensor([0.9, 0.4, 0.8]),trgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with `mnist_loss` as currently defined is that it assumes that predictions are always between 0 and 1. We need to ensure, then, that this is actually the case! As it happens, there is a function that does exactly that—let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid is one type of [activation functions](00_what_is_what.ipynb#activation-function).\n",
    "The [sigmoid](math.ipynb/#sigmoid-function) function always outputs a number between 0 and 1. It's defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch defines an accelerated version for us, so we don’t really need our own. This is an important function in deep learning, since we often want to ensure values are between 0 and 1. This is what it looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img id=\"sigmoid\" src=\"images/sigmoid.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it takes any input value, positive or negative, and smooshes it onto an output value between 0 and 1. It's also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients. \n",
    "\n",
    "Let's update `mnist_loss` to first apply `sigmoid` to the inputs and apply Binary Cross-Entropy (BCE) loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy (BCE) loss is a common loss function used for binary classification problems\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can be confident our loss function will work, even if the predictions are not between 0 and 1. All that is required is that a higher prediction corresponds to higher confidence an image is a 3.\n",
    "\n",
    "Having defined a loss function, now is a good moment to recapitulate why we did this. After all, we already had a metric, which was overall accuracy. So why did we define a loss?\n",
    "\n",
    "The key difference is that the metric is to drive human understanding and the loss is to drive automated learning. To drive automated learning, the loss must be a function that has a meaningful derivative. It can't have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level. This requirement means that sometimes it does not really reflect exactly what we are trying to achieve, but is rather a compromise between our real goal and a function that can be optimized using its gradient. The loss function is calculated for each item in our dataset, and then at the end of an epoch the loss values are all averaged and the overall mean is reported for the epoch.\n",
    "\n",
    "Metrics, on the other hand, are the numbers that we really care about. These are the values that are printed at the end of each epoch that tell us how our model is really doing. It is important that we learn to focus on these metrics, rather than the loss, when judging the performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD and Mini-Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a loss function that is suitable for driving SGD, we can consider some of the details involved in the next phase of the learning process, which is to change or update the weights based on the gradients. This is called an *optimization step*.\n",
    "\n",
    "In order to take an optimization step we need to calculate the loss over one or more data items. How many should we use? We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these is ideal. Calculating it for the whole dataset would take a very long time. Calculating it for a single item would not use much information, so it would result in a very imprecise and unstable gradient. That is, you'd be going to the trouble of updating the weights, but taking into account only how that would improve the model's performance on that single item.\n",
    "\n",
    "So instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a *mini-batch*. The number of data items in the mini-batch is called the *batch size*. A larger batch size means that you will get a more accurate and stable estimate of your dataset's gradients from the loss function, but it will take longer, and you will process fewer mini-batches per epoch. Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately. We will talk about how to make this choice throughout this book.\n",
    "\n",
    "Another good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it's helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!\n",
    "\n",
    "As we saw in our discussion of data augmentation in <<chapter_production>>, we get better generalization if we can vary things during training. One simple and effective thing we can vary is what data items we put in each mini-batch. Rather than simply enumerating our dataset in order for every epoch, instead what we normally do is randomly shuffle it on every epoch, before we create mini-batches. PyTorch and fastai provide a class that will do the shuffling and mini-batch collation for you, called `DataLoader`.\n",
    "\n",
    "<a id=\"data-loader\"></a>\n",
    "A `DataLoader` can take any Python collection and turn it into an iterator over mini-batches, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([14,  9, 12,  3,  6]),\n",
       " tensor([11,  8, 13,  7, 10]),\n",
       " tensor([1, 4, 0, 5, 2])]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = range(15)\n",
    "dl = DataLoader(coll, batch_size=5, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a model, we don't just want any Python collection, but a collection containing independent and dependent variables (that is, the inputs and targets of the model). A collection that contains tuples of independent and dependent variables is known in PyTorch as a `Dataset`. Here's an example of an extremely simple `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = list(enumerate(string.ascii_lowercase))\n",
    "ds[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we pass a `Dataset` to a `DataLoader` we will get back mini-batches which are themselves tuples of tensors representing batches of independent and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([22,  8,  2, 24, 17,  0]), ('w', 'i', 'c', 'y', 'r', 'a')],\n",
       " [tensor([10, 11,  6,  1, 20,  7]), ('k', 'l', 'g', 'b', 'u', 'h')],\n",
       " [tensor([ 3, 14,  9, 15, 16, 23]), ('d', 'o', 'j', 'p', 'q', 'x')],\n",
       " [tensor([ 4, 13, 25,  5, 12, 18]), ('e', 'n', 'z', 'f', 'm', 's')],\n",
       " [tensor([19, 21]), ('t', 'v')]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size=6, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to write our first training loop for a model using SGD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is the process of improving the prediction function by adjusting the weights and bias.\n",
    "\n",
    "Here’s how it works in our case:\n",
    "\n",
    "### Choose a model:\n",
    "\n",
    "Select the type of model to use for the problem.\n",
    "This could be a _linear model_, _decision tree_, _neural network_, or any other algorithm.\n",
    "The choice depends on the problem type (e.g., regression, classification), data characteristics, and computational constraints.\n",
    "For our case a **linear model** fits perfectly.\n",
    "\n",
    "### Make Predictions (linear model)\n",
    "\n",
    "$Prediction=(Input⋅Weights)+Bias$\n",
    "- Use the current weights and bias to compute predictions for the inputs.\n",
    "- This is where the prediction function comes into play.\n",
    "\n",
    "### Calculate Loss\n",
    "- Compare the predictions to the true labels using a loss function.\n",
    "- The loss measures how \"wrong\" the predictions are.\n",
    "\n",
    "### Adjust Weights and Bias\n",
    "- Use an optimization algorithm (like SGD) to update the weights and bias in a way that reduces the loss.\n",
    "- This involves computing gradients (derivatives of the loss with respect to the weights and bias) and applying small updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"ML Process\" width=\"400\" caption=\"ML Process\" src=\"images/ml_process.png\" id=\"matmul\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to implement the process we saw in <<gradient_descent>>. In code, our process will be implemented something like this for each epoch:\n",
    "\n",
    "```python\n",
    "for x,y in dl:\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, y)\n",
    "    loss.backward()\n",
    "    parameters -= parameters.grad * lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's re-initialize our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Prepare the data](#preparing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Training and Validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data stacked\n",
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
    "# Labels\n",
    "train_y = torch.tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
    "\n",
    "# For test test we do the same\n",
    "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
    "valid_y = torch.tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it [PyTorch ready](#preparing_data_step_3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dataset in PyTorch is required to return a tuple of `(x,y)` when indexed.\n",
    "# We create a tuple from our x and y.\n",
    "train_dataset = list(zip(train_x,train_y))\n",
    "valid_dataset = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to PyTorch [DataLoader](#data-loader) for batch-sizing and shuffleing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For shuffling and mini-batch collation we need to create a DataLoader from our DataSets.\n",
    "# A DataLoader can take any Python collection and turn it into an iterator over mini-batches, like so:\n",
    "train_dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "test_dataloadet = DataLoader(valid_dataset, batch_size=256, shuffle=True)\n",
    "# iter(dl) function is a built-in Python function. It creates an iterator from the DataLoader (dl), which allows you to retrieve batches of data.\n",
    "# The next function is a built-in Python function. It retrieves the next item from an iterator.\n",
    "train_features, train_labels = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cweights with size (28*28, 1) because:\n",
    "# - Each image is flattened into a vector of 784 pixels (features),\n",
    "# - We need one weight for each pixel to compute a weighted sum,\n",
    "# - The model produces a single output (e.g., a prediction for binary classification).#\n",
    "weights = init_params((28*28, 1))\n",
    "\n",
    "# This bias adds flexibility to the model by allowing it to shift predictions up or down, \n",
    "# ensuring it can fit data even when inputs are zero. \n",
    "# Like weights, the bias is updated during training to minimize the loss.\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Predict\n",
    "We chose a simple [linear model](optional_read.ipynb#linear-model) for this prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_r1_model_predict(xb): return xb@weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with a mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = train_x[:5]\n",
    "mini_batch_prediction = linear_r1_model_predict(mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Measure the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the loss using [Binary Cross-Entropy](optional_read.ipynb#binary-cross-entropy) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2375, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mnist_loss(mini_batch_prediction, train_y[:5])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Calculate the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function to automate the task of calculating gradient for each input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_calculate_gradient(input_batch, label_batch, model):\n",
    "    # Step 1: Make predictions using the model\n",
    "    predictions = model(input_batch)\n",
    "    # Step 2: Compute the loss by comparing predictions to true labels\n",
    "    loss = mnist_loss(predictions, label_batch)\n",
    "    # Step 3: Compute gradients of the loss with respect to the model's parameters\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), tensor(-0.0050), tensor([-0.0316]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predict_and_calculate_gradient(mini_batch, train_y[:5], linear_r1_model_predict)\n",
    "# Current gradients\n",
    "weights.grad.shape,weights.grad.mean(), bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the gradient again the value of our gradients will differ from the first time. The reason for this is that `loss.backward` actually *adds* the gradients of `loss` to any gradients that are currently stored. So, we have to set the current gradients to 0 first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), tensor(-0.0099), tensor([-0.0631]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_and_calculate_gradient(mini_batch, train_y[:5], linear_r1_model_predict)\n",
    "# Current gradients\n",
    "weights.grad.shape,weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad.zero_()\n",
    "bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note: Inplace Operations: Methods in PyTorch whose names end in an underscore modify their objects _in place_. For instance, `bias.zero_()` sets all elements of the tensor `bias` to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Optimize and Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our only remaining step is to update the weights and biases based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the `data` attribute of a tensor then PyTorch will not take the gradient of that step. Here's our basic training loop for an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataset, ml_model, learning_rate, params):\n",
    "    # Loop through each batch of inputs and labels in the dataset\n",
    "    for input, label in dataset:\n",
    "        # Step 1: Compute predictions and calculate gradients\n",
    "        predict_and_calculate_gradient(input, label, ml_model)\n",
    "        \n",
    "        # Step 2: Update each parameter (weight/bias) based on the gradient\n",
    "        for param in params:\n",
    "            # Update the parameter using the gradient and learning rate\n",
    "            # New Parameter=Old Parameter−(Gradient×Learning Rate)\n",
    "            param.data -= param.grad * learning_rate\n",
    "            \n",
    "            # Reset the gradient to zero for the next batch\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is `p.data`?\n",
    "- `p` is a parameter tensor (e.g., weights or biases) in your model.\n",
    "- `p.data` refers to the actual numerical values stored in the tensor, without any gradient-tracking behavior.\n",
    "\n",
    "- For example, if p represents a weight tensor, `p.data` contains the current values of the weights.\n",
    "\n",
    "When you modify p.data, you are directly changing the values of the weights or biases without telling PyTorch to track this change in the computation graph. This is important because we don’t want PyTorch to compute gradients for this update step itself—it would complicate things when computing gradients for the next batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to check how we're doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0. So our accuracy for each item can be calculated (using broadcasting, so no loops!) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mini_batch_prediction>0.0).float() == train_y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us this function to calculate our validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(input, label):\n",
    "    predictions_sigmoid = input.sigmoid()\n",
    "    correct = (predictions_sigmoid>0.5) == label\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_accuracy(linear_r1_model_predict(mini_batch), train_y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then put the batches together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(input), label) for input,label in test_dataloadet]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5648"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear_r1_model_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our starting point. Let's train for one epoch, and see if the accuracy improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5098"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1.\n",
    "params = weights,bias\n",
    "train_epoch(train_dataset, linear_r1_model_predict, lr, params)\n",
    "validate_epoch(linear_r1_model_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The do a few more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5855 0.7546 0.786 0.7955 0.7925 0.8185 0.8738 0.8973 0.8852 0.899 0.9098 0.8984 0.9058 0.9103 0.9327 0.921 0.9053 0.9156 0.9245 0.9368 "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    train_epoch(train_dataset, linear_r1_model_predict, lr, params)\n",
    "    print(validate_epoch(linear_r1_model_predict), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! We're already about at the same accuracy as our \"pixel similarity\" approach, and we've created a general-purpose foundation we can build on. Our next step will be to create an object that will handle the SGD step for us. In PyTorch, it's called an *optimizer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is such a general foundation, PyTorch provides some useful classes to make it easier to implement. The first thing we can do is replace our `linear1` function with PyTorch's `nn.Linear` module. A *module* is an object of a class that inherits from the PyTorch `nn.Module` class. Objects of this class behave identically to standard Python functions, in that you can call them using parentheses and they will return the activations of a model.\n",
    "\n",
    "`nn.Linear` does the same thing as our `init_params` and `linear` together. It contains both the *weights* and *biases* in a single class. Here's how we replicate our model from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every PyTorch module knows what parameters it has that can be trained; they are available through the `parameters` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to create an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our optimizer by passing in the model's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop can now be simplified to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataset, model):\n",
    "    for xb, yb in dataset:\n",
    "        predict_and_calculate_gradient(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our little training loop in a function, to make things simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(train_dataset, model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same as in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5044 0.525 0.7022 0.6429 0.6363 0.7901 0.7907 0.8336 0.8394 0.8647 0.8728 0.8268 0.8636 0.8457 0.8469 0.87 0.8755 0.8984 0.8929 0.9186 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have a general procedure for optimizing the parameters of a function, and we have tried it out on a very boring function: a simple linear classifier. A linear classifier is very constrained in terms of what it can do. To make it a bit more complex (and able to handle more tasks), we need to add something nonlinear between two linear classifiers—this is what gives us a neural network.\n",
    "\n",
    "Here is the entire definition of a basic neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_net(xb): \n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! All we have in `simple_net` is two linear classifiers with a `max` function between them.\n",
    "\n",
    "Here, `w1` and `w2` are weight tensors, and `b1` and `b2` are bias tensors; that is, parameters that are initially randomly initialized, just like we did in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key point about this is that `w1` has 30 output activations (which means that `w2` must have 30 input activations, so they match). That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that `30` to anything you like, to make the model more or less complex.\n",
    "\n",
    "That little function `res.max(tensor(0.0))` is called a *rectified linear unit*, also known as *ReLU*. We think we can all agree that *rectified linear unit* sounds pretty fancy and complicated... But actually, there's nothing more to it than `res.max(tensor(0.0))`—in other words, replace every negative number with a zero. This tiny function is also available in PyTorch as `F.relu`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD7CAYAAABt0P8jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhU5fn/8fctW1hlC7iBCAUUkDXivlRtcZcWtbK4tFYURau2Vluh4lK3trZqKZZvtaggLgiudalVvu7aEDaDEJQdBMIWk7AEkvv7x0x+v3FMyAmZJTPzeV3XXMyc85xz7nOYuXPmOWfux9wdERHJHPslOwAREUksJX4RkQyjxC8ikmGU+EVEMowSv4hIhmmY7ACCaN++vXfp0iXZYYiIpJQ5c+Zscvfs6Okpkfi7dOlCbm5ussMQEUkpZrayqunq6hERyTBK/CIiGUaJX0Qkwyjxi4hkGCV+EZEMU2PiN7MmZvaYma00s2Izm2tmZ+6l/Y1mtt7MiszscTNrEjGvi5m9a2bbzWyxmZ0eqx0REZFggpzxNwRWAycD+wPjgefMrEt0QzMbAtwKnAZ0AboCd0Q0mQ7MBdoBtwEzzOw795iKiEj81Jj43b3U3Se4+wp3r3D3V4HlwKAqml8GPObu+e6+FbgLuBzAzHoAA4Hb3X2Hu78ALASGxWhfRETSxleFJfzxzSXsKa+I+bpr3cdvZh2BHkB+FbN7A/MjXs8HOppZu/C8Ze5eHDW/dzXbGW1muWaWW1hYWNswRURS1vayPYyZOoenP1vF5tKymK+/VonfzBoB04An3H1xFU1aAEURryuft6xiXuX8llVty90nu3uOu+dkZ6s3SEQyg7tz26zPWbqxhIcu7k/HVlkx30bgxG9m+wFPAWXA2GqalQCtIl5XPi+uYl7l/GJERASAaZ+uYtbctdx4eg9O7B6fk95Aid/MDHgM6AgMc/fd1TTNB/pFvO4HbHD3zeF5Xc2sZdT8qrqMREQyzoI127jzlUWc0jObsd//Xty2E/SMfxJwBHCuu+/YS7sngSvMrJeZtQHGAVMA3L0AmAfcbmZZZvYjoC/wwr4GLyKSLraWljFmah7ZLZvw54v6s99+FrdtBbmP/1DgKqA/sN7MSsKPkWbWOfy8M4C7vwE8ALwLrAw/bo9Y3cVADrAVuA+4wN115VZEMlpFhXPjc/MoLN7F30YOpE3zxnHdXo1lmd19JbC3Pz0toto/CDxYzbpWAKcED09EJP1NfPdLZi8p5K6hfejXqXXct6eSDSIiSfTB0k08+HYBQ/sfxKijOydkm0r8IiJJ8nXRDq5/Zi7dO7Tgnh8fSeg+mvhT4hcRSYKyPRVcMy2PXbvLmTRqEM0aJ25AxJQYelFEJN3c868vmLtqGxNHDKRbdouaF4ghnfGLiCTYqwvWMeWjFfz0+C6c3ffAhG9fiV9EJIG+3FjCLTMWMLBza35z5hFJiUGJX0QkQUp3hYqvNWnUgIkjB9K4YXJSsPr4RUQSwN357ayFfFlYwlM/O5oD92+atFh0xi8ikgBTP1nJS/PWcdPpPTihe/ukxqLELyISZ/NWb+POVxfx/Z7ZXBvH4mtBKfGLiMTR1tIyrp2WR4eWWfz5J/EtvhaU+vhFROKkosK54dlQ8bUZY46ldbP4Fl8LSmf8IiJx8sg7X/K/BYXcfl4v+h4S/+JrQSnxi4jEwXsFhfzlPwX8eMDBjBicmOJrQSnxi4jE2NptO/jFM3Pp0aElv/9R4oqvBRV06MWxZpZrZrvMbMpe2j0aMVBLSbh9ccT82Wa2M2L+khjsg4hIvVG2p4Jrp+Wxu9yZNGogTRs3SHZI3xH04u464G5gCFDtrw7c/Wrg6srX4T8SFVHNxrr7P2oXpohIavj9a4uYt3obfxs5kK4JLr4WVKDE7+4zAcwsBzgkyDJm1hwYBpyzz9GJiKSQl+ev44mPV/LzEw7jrCMTX3wtqHj28Q8DCoH3oqbfa2abzOxDMzuluoXNbHS4eym3sFDD8opI/bZ0QzG3vrCAnEPbcMuZhyc7nL2KZ+K/DHjS3T1i2i1AV+BgYDLwipl1q2phd5/s7jnunpOdnR3HMEVE6qZ01x7GTMujWeNQ8bVGDer3fTNxic7MOgEnA09GTnf3T9292N13ufsTwIfAWfGIQUQkEdydW2cuZFlhCQ8PH0DHVlnJDqlG8fqzdCnwkbsvq6GdA/XrPicRkVp48uOVvDJ/Hb/8YU+O65bc4mtBBb2ds6GZZQENgAZmlmVme7swfCkwJWodrc1sSOWyZjYSOAl4cx9jFxFJqrxVW7n7tUWcengHxpxcZa91vRT0jH8csAO4FRgVfj7OzDqH78f/fz9LM7NjCd3583zUOhoRuiW0ENgEXAcMdXfdyy8iKWdLaRljp+XRsVUWf76ofhRfCyro7ZwTgAnVzP7Wjaru/jHQvIp1FAJH1S48EZH6p7zC+cUzc9lUWsbMMcexf7NGyQ6pVur3pWcRkXro4f8s5f2lm7jjvN70OXj/ZIdTa0r8IiK1MHvJRh5+ZynDBh7CxUd1SnY4+0SJX0QkoDVbt3PDs/Po2bEldw/tU++KrwWlxC8iEsCuPeVcOy2P8nJn0qhB9bL4WlAagUtEJIC7X/2C+WuKeHTUIA5r/537V1KKzvhFRGrw0ry1PPXJSkaf1JUz+hyQ7HDqTIlfRGQvCjYUc+sLCxncpS03D+mZ7HBiQolfRKQaJbv2cPXUOTRv0pBHRgyo98XXgkqPvRARiTF355YXFrBiUykPD++fEsXXglLiFxGpwpSPVvDagq+5ecjhKVN8LSglfhGRKHNWbuX3r33B6Ud05OqTuyY7nJhT4hcRibC5ZBdjn87joNZN+dNF/VL2R1p7o/v4RUTCQsXX5rG5svha09QqvhaUzvhFRMIeeruAD77cxF3np2bxtaCU+EVEgHcXb+Thd77kwkGH8JOjOte8QAoLOgLXWDPLNbNdZjZlL+0uN7Py8OAslY9TIuZ3MbN3zWy7mS02s9PrvgsiInWzekuo+NoRB7birqF9kh1O3AXt419HaPSsIUDTGtp+7O4nVDNvOvAxoQHWzwJmmFn38CAtIiIJt2tPOdc+nUdFhTNp5ECyGqVu8bWgAp3xu/tMd38R2LyvGzKzHsBA4HZ33+HuLwALgWH7uk4Rkbq685VFLFhTxB8v6keXFC++FlQ8+vgHmNkmMysws/ERg7L3Bpa5e3FE2/nh6d9hZqPD3Uu5hYX6QiAisTczbw3TPl3FVSd1ZUjv1C++FlSsE/97QB+gA6Ez+eHAzeF5LYCiqPZFQMuqVuTuk909x91zsrOzYxymiGS6xeu/4bezFjL4sPQpvhZUTBO/uy9z9+XuXuHuC4E7gQvCs0uAVlGLtAKKERFJoOKduxkzNY+WWY3464gBNEyT4mtBxXtvHaj82Vs+0NXMIs/w+4Wni4gkhLvz6xkLWLVlO38dPoAOLdOn+FpQQW/nbGhmWUADoIGZZUX03Ue2O9PMOoafHw6MB14CcPcCYB5we3j5HwF9gRdisysiIjV77IPlvP75en49pCdHd22X7HCSIugZ/zhgB3ArMCr8fJyZdQ7fq1/5a4fTgAVmVgr8C5gJ3BOxnouBHGArcB9wgW7lFJFEyV2xhfteX8wPe3Vk9EnpV3wtKHP3ZMdQo5ycHM/NzU12GCKSwjaV7OLsh98nq1EDXh57QtrW4YlkZnPcPSd6uoq0iUjaK69wrp8+l23bdzPrmsEZkfT3RolfRNLeg/9ewkdfbeaBC/rS66DomwszT2bdwyQiGeedxRuY+O5X/CSnExfldEp2OPWCEr+IpK3VW7ZzwzPz6HVgK+44v8oiARlJiV9E0tLO3eVcMy0PBx4dNSgjiq8FpT5+EUlLd7yyiIVri/ifS3Po3K5ZssOpV3TGLyJp54U5a5j+2SquPrkbP+jVMdnh1DtK/CKSVhav/4bbXlzIMV3b8qsf9kh2OPWSEr+IpI3K4mutshrxyPCBGVd8LSj18YtIWnB3bn4+VHxt+pXHkN2ySbJDqrf051BE0sJjHyznjfz13HrG4Qw+rG2yw6nXlPhFJOV9tnwL976+mDN6H8DPTzws2eHUe0r8IpLSNhbvZOzTeXRq05QHLuyLmdW8UIZTH7+IpKw95RVcP30u3+zczRM/G0yrrMwuvhZU0IFYxoYHPt9lZlP20u4yM5tjZt+Y2RozeyBywBYzm21mO8M1/EvMbEkM9kFEMtSf/l3AJ8u2cPfQIzniQBVfCypoV8864G7g8RraNQNuANoDRxMamOVXUW3GunuL8COzRjgWkZj596INTJr9FcMHd+KCQYckO5yUEqirx91nAphZDlDtEXb3SREv15rZNOD7dYpQRCTKqs3buem5efQ5uBW3n6via7UV74u7J/HdwdTvNbNNZvahmZ1S3YJmNjrcvZRbWKjRGUUkZOfucsZMm4MBk0aq+Nq+iFviN7OfEhpf948Rk28BugIHA5OBV8ysW1XLu/tkd89x95zs7Ox4hSkiKWbCy/nkr/uGv1zcn05tVXxtX8Ql8ZvZUEKDqZ/p7psqp7v7p+5e7O673P0J4EPgrHjEICLp57nc1Tzz39Vc+/1unHq4iq/tq5jfzmlmZwD/A5zt7gtraO6AbroVkRrlryti/Iufc1y3dtz0A90XUhdBb+dsaGZZQAOggZllRd6mGdHuVGAaMMzdP4ua19rMhlQua2YjCV0DeLPuuyEi6axox26umZZH62aNeHj4ABrsp/PFugja1TMO2AHcCowKPx9nZp3D9+N3DrcbD+wP/CviXv3Xw/MaEboltBDYBFwHDHV33csvItUKFV+bz9qtO5g4YiDtW6j4Wl0FvZ1zAjChmtktItpVe+umuxcCR9UiNhERJr+3jLcWbWDc2UeQ00XF12JBtXpEpN76dNlmHnhzCWcdeQBXnKDia7GixC8i9dLGb3YydvpcDm3bjPuHqfhaLKlIm4jUO3vKK7hu+lxKdu5h6hVH01LF12JKiV9E6p0/vLWET5dv4c8/6UfPA1omO5y0o64eEalX3spfz9//dxkjj+7Mjwao+Fo8KPGLSL2xcnMpv3x+Pn0P2Z/fndsr2eGkLSV+EakXdu4uZ8zUPPYzY+KIgTRpqOJr8aI+fhGpF3730ucs+vob/nn5USq+Fmc64xeRpHv2v6t4LncN1536Pb5/eIdkh5P2lPhFJKk+X1vE+JfyOeF77bnh9B7JDicjKPGLSNJUFl9r26wxD13cX8XXEkR9/CKSFBUVzi+fm8+6bTt49qpjaKfiawmjM34RSYq/v7eMt7/YwG/POoJBh6r4WiIp8YtIwn381Wb+8OZizu57ID89vkuyw8k4SvwiklAbv9nJddPn0qV9cxVfS5KgI3CNNbNcM9tlZlNqaHujma03syIze9zMmkTM62Jm75rZdjNbbGan1zF+EUkhu8srGPv0XEp37eHRUYNo0USXGZMh6Bn/OkKjZz2+t0ZmNoTQKF2nAV2ArsAdEU2mA3OBdsBtwAwzy65dyCKSqv7w5hI+W7GF+4YdSY+OKr6WLIESv7vPdPcXgc01NL0MeMzd8919K3AXcDmAmfUABgK3u/sOd38BWAgM29fgRSR1vPH5eia/t4xLjjmU8/sfnOxwMlqs+/h7A/MjXs8HOppZu/C8Ze5eHDW/d1UrMrPR4e6l3MLCwhiHKSKJtGJTKTc/P59+nVoz7pwjkh1Oxot14m8BFEW8rnzesop5lfOr/L7n7pPdPcfdc7Kz1Rskkqp2lJVz9dQ5NGhgTBwxQMXX6oFYX1kpAVpFvK58XlzFvMr5xYhIWnJ3xr/0OUs2FPPPy4/ikDYqvlYfxPqMPx/oF/G6H7DB3TeH53U1s5ZR8/NjHIOI1BPP/nc1M+as4bpTu3NKTxVfqy+C3s7Z0MyygAZAAzPLMrOqvi08CVxhZr3MrA0wDpgC4O4FwDzg9vDyPwL6Ai/EYD9EpJ75fG0Rv3s5nxO7t+cXp3VPdjgSIegZ/zhgB6FbNUeFn48zs85mVmJmnQHc/Q3gAeBdYGX4cXvEei4GcoCtwH3ABe6uK7ciaaZo+26unjqHds0b89DFA1R8rZ4xd092DDXKycnx3NzcZIchIgFUVDhXPpnLe0sLee6qYxnQuU2yQ8pYZjbH3XOip6tkg4jE1KPvfcV/Fm9k3Nm9lPTrKSV+EYmZj77axB/fXMK5/Q7i0mMPTXY4Ug0lfhGJifVFO7l++lwOa9+c+358pIqv1WOqkCQidRYqvpbH9rJypl95DM1VfK1e0/+OiNTZ/a8vJnflVh4ePoDuKr5W76mrR0Tq5PWFX/OPD5Zz2bGHcl6/g5IdjgSgxC8i+2xZYQk3z1hA/06tue3sXskORwJS4heRfbKjrJxrpuXRqIExceRAGjdUOkkV6uMXkVpzd257cSFLNhTzxE8Hc3DrpskOSWpBf6JFpNamf7aamXlr+cVp3Tmph8qmpxolfhGplQVrtjHh5XxO6pHN9aeq+FoqUuIXkcC2bS9jzNQ82rdozF9+0p/9VHwtJamPX0QCqahwbnx2HhuLd/L81cfRtnnjZIck+0hn/CISyN9mf8m7SwoZf04v+ndqnexwpA6U+EWkRh9+uYkH/13Aef0O4pJjVHwt1QUdgautmc0ys1IzW2lmI6pp93p4YJbKR5mZLYyYv8LMdkTMfytWOyIi8VFZfK1rdgvuVfG1tBC0j38iUAZ0BPoDr5nZfHf/1ni57n5m5Gszmw28E7Wuc9397X0LV0QSaXd5Bdc+ncfO3eU8OmqQiq+liRrP+M2sOTAMGO/uJe7+AfAycEkNy3UBTgSeqnuYIpIM9/5rMXNWbuX+C/ryvQ4tkh2OxEiQrp4eQHl4sPRK84HeNSx3KfC+uy+Pmj7NzArN7C0z61fdwmY22sxyzSy3sFDD8ook2msLvubxD5dz+XFdOKeviq+lkyCJvwVQFDWtCKip9uqlwJSoaSOBLsChhAZkf9PMqrw9wN0nu3uOu+dkZ+uXgSKJ9FVhCb+eMZ+BnVvz27OOSHY4EmNBEn8J0CpqWiuguLoFzOwE4ABgRuR0d//Q3Xe4+3Z3vxfYRqg7SETqie1lexgzdQ5NGjVQ8bU0FeR/tABoaGaRv83uB+RX0x7gMmCmu5fUsG4HdIuASD3h7tw263OWbizhoYv7c+D+Kr6WjmpM/O5eCswE7jSz5mZ2PHA+1Vy0NbOmwIVEdfOYWWczO97MGptZlpndDLQHPqzjPohIjEz7dBWz5q7lxtN7cGJ3dbGmq6Df4a4BmgIbgenAGHfPN7MTzSz6rH4ooWsA70ZNbwlMArYCa4EzgDPdffO+Bi8isTN/9TbufGURp/TMZuz3v5fscCSOzN2THUONcnJyPDc3N9lhiKStraVlnPPIBwC8et0JtFEdnrRgZnPcPSd6un6NIZLhKiqcG5+bR2HxLp6/+lgl/Qygy/UiGe6v737J7CWFjD+3F/1UfC0jKPGLZLD3lxby57cLGNr/IEYd3TnZ4UiCKPGLZKh123bwi2fm0b1DC+5R8bWMosQvkoHK9lQw9uk8du0uZ9KoQTRrrMt9mUT/2yIZ6J5/fUHeqm1MHDGQbtkqvpZpdMYvkmFemb+OKR+t4GfHH8bZfQ9MdjiSBEr8Ihnky40l3PrCAgYd2obfnHV4ssORJFHiF8kQpbtCxdeyGjVg4oiBNGqgj3+mUh+/SAZwd347ayFfFZbw1BVHc8D+WckOSZJIf/JFMsDUT1by0rx13PSDHhz/vfbJDkeSTIlfJM3NXbWVO19dxKmHd+CaU1R8TZT4RdLaltIyrp2WR8dWWTx4UT/2208/0hL18YukrfIK54Zn57GppIwZY46ldTMVX5OQQGf8ZtbWzGaZWamZrTSzEdW0m2Bmu82sJOLRNWJ+fzObY2bbw//2j9WOiMi3PfLOUt4rKOT283rR9xAVX5P/L2hXz0SgDOhIaMD0SWbWu5q2z7p7i4jHMgAzawy8BEwF2gBPAC+Fp4tIDP1vQSEP/WcpPx5wMCMGq/iafFuNid/MmgPDgPHuXuLuHwAvA5fUclunEOpa+ou773L3hwmNt3tqLdcjInuxdtsObnhmLj06tOT3P1LxNfmuIGf8PYBydy+ImDYfqO6M/1wz22Jm+WY2JmJ6b2CBf3vIrwXVrcfMRptZrpnlFhYWBghTRHbtKeeaaXnsLncmjRpI08YNkh2S1ENBEn8LQmPoRioiNIZutOeAI4Bs4Ergd2Y2fB/Wg7tPdvccd8/JztagzyJB/P61L5i/eht/vLAvXVV8TaoRJPGXAK2iprUCiqMbuvsid1/n7uXu/hHwEHBBbdcjIrX30ry1PPnxSn5+wmGc0UfF16R6QRJ/AdDQzLpHTOsH5AdY1gn14xNu39e+3eHYN+B6RGQvlm4o5jczF3JUlzbccqaKr8ne1Zj43b0UmAncaWbNzex44Hzgqei2Zna+mbWxkMHA9YTu5AGYDZQD15tZEzMbG57+Tgz2QyRjlezaw9VT59CscQMeGa7ia1KzoO+Qa4CmwEZgOjDG3fPN7EQzK4lodzHwJaHumyeB+939CQB3LwOGApcC24CfAUPD00VkH7g7t76wgOWbSnl4+AAVX5NAAv1y1923EEra0dPfJ3TRtvL18Og2Ue3nAoNqGaOIVOOJj1bw6oKvuXlIT47rpuJrEoy+E4qkqDkrt3L3a19w2uEdGHNyt2SHIylEiV8kBW0u2cXYp/M4sHUWD17UX8XXpFZUpE0kxVQWX9tcWsbMMcexf7NGyQ5JUozO+EVSzEP/Wcr7Szdxx3m96XPw/skOR1KQEr9ICpm9ZCOPvLOUYQMP4eKjOiU7HElRSvwiKWLN1u3c8Ow8enZsyd1D+6j4muwzJX6RFFBZfK283Jk0apCKr0md6OKuSAq469VFLFhTxKOjBnFY++bJDkdSnM74Req5F+euZeonqxh9UlfO6HNAssORNKDEL1KPFYSLrw3u0pabh/RMdjiSJpT4ReqpyuJrzZs05K8jBqj4msSM3kki9ZC7c8uMBazYVMojwwfQoZWKr0nsKPGL1EP//HAFry38mpuHHM6x3dolOxxJM0r8IvVM7oot3POvLzj9iI5cdVLXZIcjaUiJX6Qe2VSyi2ufzuOg1k3500X9VHxN4iJQ4jeztmY2y8xKzWylmY2opt3NZva5mRWb2XIzuzlq/goz22FmJeHHW7HYCZF0UF7h/OKZuWzdvpu/jRzI/k1VfE3iI+gPuCYCZUBHoD/wmpnNd/fo8XKN0AhbC4BuwFtmttrdn4loc667v13HuEXSzp//XcCHX27m/mFHqviaxFWNZ/xm1hwYBox39xJ3/wB4Gbgkuq27P+Duee6+x92XEBpv9/hYBy2Sbt5ZvIG/vvslFw46hJ8c1TnZ4UiaC9LV0wMod/eCiGnzgd57W8hCFaROBKK/FUwzs0Ize8vM+u1l+dFmlmtmuYWFhQHCFElNq7ds58Zn53PEga24a2ifZIcjGSBI4m8BFEVNKwJa1rDchPD6/xkxbSTQBTgUeBd408xaV7Wwu0929xx3z8nOzg4Qpkjq2bk7VHytwp1HRw0kq5GKr0n8BUn8JUCrqGmtgOLqFjCzsYT6+s92912V0939Q3ff4e7b3f1eYBuhbwUiGenOVxexcG0Rf7qwH4e2U/E1SYwgib8AaGhm3SOm9eO7XTgAmNnPgFuB09x9TQ3rdkIXhEUyzsy8NTz96SquPrkbP+yt4muSODUmfncvBWYCd5pZczM7HjgfeCq6rZmNBO4BfuDuy6LmdTaz482ssZllhW/1bA98GIsdEUkli9d/w29nLeSYrm351Q97JDscyTBBf8B1DdAU2AhMB8a4e76ZnWhmJRHt7gbaAf+NuFf/0fC8lsAkYCuwFjgDONPdN8diR0RSRfHO3YyZmkerrEY8PHwADVV8TRIs0H387r4FGFrF9PcJXfytfH3YXtaRD/TdhxhF0oa78+sZC1i1ZTvTrzyGDi1VfE0ST6caIgn02AfLef3z9dxyRk8GH9Y22eFIhlLiF0mQ/67Ywr2vL2ZI745ceaKKr0nyKPGLJEBh8S6unZZHpzZN+cOF/Qj9vlEkOTTYukic7Smv4PrpcynasZspPx1MqywVX5PkUuIXibMH/13Ax8s284cL+tLroOjfQooknrp6ROLo7UUb+Nvsr7j4qE5cmNMp2eGIAEr8InGzavN2bnpuHr0PasWE8/Za01AkoZT4ReJg5+5yrnl6DgCTRg5S8TWpV9THLxIHd7ySz+drv+Efl+bQuV2zZIcj8i064xeJsRlz1jD9s9Vcc0o3Tu/VMdnhiHyHEr9IDH3x9TfcNmshx3Ztx00/UPE1qZ+U+EVi5JuduxkzdQ77N1XxNanf1McvEgPuzq+fX8DqrTt4ZvQxZLdskuyQRKqlUxKRGPjH+8t5I389vznzcI7qouJrUr8p8YvU0afLNnPfG4s5s88BXHFCtZXJReqNQInfzNqa2SwzKzWzlWY2opp2Zmb3m9nm8OMBi6hGZWb9zWyOmW0P/9s/VjsikgyfLNvMtU/PpXPbZjxwQV8VX5OUEPSMfyJQBnQERgKTzKyqnyKOJjRgSz9Cg66cA1wFYGaNgZeAqUAb4AngpfB0kZRSvHM3t81ayMWTP6FZ4wb8/ZJBtFTxNUkRNV7cNbPmwDCgj7uXAB+Y2cvAJYQGVY90GfCnykHWzexPwJXAo8Ap4e39xd0deNjMfgWcCrwRm935tp8/8V9Wbt4ej1VLhttUsouiHbv5+QmH8csf9qRpY/0yV1JHkLt6egDl7l4QMW0+cHIVbXuH50W26x0xb0E46VdaEJ7+ncRvZqMJfYOgc+fOAcL8rs5tm9O4oS5jSOz1PqgVlx3XhQGd2yQ7FJFaC5L4WwBFUdOKCA2eXlPbIqBFuJ+/NuvB3ScDkwFycnK8qjY1+d25vfZlMRGRtBbkdLgEiC4i3gooDtC2FVASPsuvzXpERCROgiT+AqChmXWPmNYPyK+ibX54XlXt8oG+9u3bHvpWsx4REYmTGhO/u5cCM4E7zay5mR0PnA88VUXzJ4GbzOxgMzsI+CUwJTxvNlAOXG9mTcxsbHj6O9VBRU4AAAUOSURBVHXbBRERqY2gVz6vAZoCG4HpwBh3zzezE82sJKLd34FXgIXA58Br4Wm4exmhWz0vBbYBPwOGhqeLiEiC2LdvsqmfcnJyPDc3N9lhiIikFDOb4+450dN1r6OISIZR4hcRyTBK/CIiGSYl+vjNrBBYuY+Ltwc2xTCcWFFctaO4akdx1U66xnWou2dHT0yJxF8XZpZb1cWNZFNctaO4akdx1U6mxaWuHhGRDKPELyKSYTIh8U9OdgDVUFy1o7hqR3HVTkbFlfZ9/CIi8m2ZcMYvIiIRlPhFRDKMEr+ISIZJq8QfLvf8mJmtNLNiM5trZmfWsMyNZrbezIrM7HEzaxKn2MaaWa6Z7TKzKTW0vdzMys2sJOJxSrLjCrdP1PFqa2azzKw0/P85Yi9tJ5jZ7qjj1TWRcVjI/Wa2Ofx4IGrsiZirRWxxOz5VbKs27/OEvJdqE1ciP3vh7dUqZ8XqmKVV4ic0lORqQuMB7w+MB54zsy5VNTazIYQGjD8N6AJ0Be6IU2zrgLuBxwO2/9jdW0Q8Zic7rgQfr4lAGdARGAlMMrPee2n/bNTxWpbgOEYTKjvej9AAQ+cAV8UohrrGBvE7PtECvZ8S/F4KHFdYoj57UIucFdNj5u5p/SA0oPuwauY9DdwT8fo0YH2c47kbmFJDm8uBDxJ8nILElZDjBTQnlNB6REx7CrivmvYTgKnJjAP4CBgd8foK4JM4/n/VJra4HJ+6vJ+S8dkLGFfCP3tVxFBlzorlMUu3M/5vMbOOQA+qH96xNzA/4vV8oKOZtYt3bAEMMLNNZlZgZuPNrGGyAyJxx6sHUO7uBVHb2tsZ/7lmtsXM8s1sTBLiqOrY7C3eRMYG8Tk+daHPXhVqyFkxO2Zpm/jNrBEwDXjC3RdX06wFUBTxuvJ5y3jGFsB7QB+gAzAMGA7cnNSIQhJ1vKK3U7mt6rbzHHAEkA1cCfzOzIYnOI6qjk2LOPbz1ya2eB2futBnL0qAnBWzY5ZSid/MZpuZV/P4IKLdfoS+9pYBY6tdIZQArSJeVz4vjkdcQbn7Mndf7u4V7r4QuBO4oLbriXVcJO54RW+ncltVbsfdF7n7Oncvd/ePgIfYh+NVhdrEUdWxKfHwd/I4CBxbHI9PXcTkvRRrsfrs1VbAnBWzY5ZSid/dT3F3q+ZxAoTurgAeI3TBa5i7797LKvMJXYyr1A/Y4O6bYx1XHTlQ6zPHOMSVqONVADQ0s+5R26quy+47m2AfjlcVahNHVccmaLzxji1arI5PXcTkvZQAcT9WtchZMTtmKZX4A5pE6Gvtue6+o4a2TwJXmFkvM2sDjAOmxCMoM2toZllAA6CBmWVV13doZmeG+/ows8MJXel/KdlxkaDj5e6lwEzgTjNrbmbHA+cTOiOqah/ON7M2FjIYuJ4YHK9axvEkcJOZHWxmBwG/JE7vpdrGFq/jU5VavJ8S9tmrTVyJ/OxFCJqzYnfMknn1OtYP4FBCf6F3EvpaVPkYGZ7fOfy6c8QyNwEbgG+AfwJN4hTbhHBskY8JVcUF/DEcUymwjNDXzUbJjivBx6st8GL4GKwCRkTMO5FQN0rl6+nA5nCsi4Hr4x1HFTEY8ACwJfx4gHAtrDi+34PGFrfjE/T9lMz3Um3iSuRnL7y9anNWPI+ZirSJiGSYdOzqERGRvVDiFxHJMEr8IiIZRolfRCTDKPGLiGQYJX4RkQyjxC8ikmGU+EVEMsz/AXPhmYaLqaN6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(F.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there's no point just putting one linear layer directly after another one, because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.\n",
    "\n",
    "But if we put a nonlinear function between them, such as `max`, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work. The `max` function is particularly interesting, because it operates as a simple `if` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> S: Mathematically, we say the composition of two linear functions is another linear function. So, we can stack as many linear classifiers as we want on top of each other, and without nonlinear functions between them, it will just be the same as one linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for `w1` and `w2` and if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it closer to the wiggly function, we just have to use shorter lines. This is known as the [Universal Approximation Theorem](00_what_is_what.ipynb#uni_apprx_theo). The three lines of code that we have here are known as *layers*. The first and third are known as *linear layers*, and the second line of code is known variously as a *nonlinearity*, or *activation function*.\n",
    "\n",
    "Just like in the previous section, we can replace this code with something a bit simpler, by taking advantage of PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` creates a module that will call each of the listed layers or functions in turn.\n",
    "\n",
    "`nn.ReLU` is a PyTorch module that does exactly the same thing as the `F.relu` function. Most functions that can appear in a model also have identical forms that are modules. Generally, it's just a case of replacing `F` with `nn` and changing the capitalization. When using `nn.Sequential`, PyTorch requires us to use the module version. Since modules are classes, we have to instantiate them, which is why you see `nn.ReLU()` in this example. \n",
    "\n",
    "Because `nn.Sequential` is a module, we can get its parameters, which will return a list of all the parameters of all the modules it contains. Let's try it out! As this is a deeper model, we'll use a lower learning rate and a few more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have something that is rather magical:\n",
    "\n",
    "1. A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters\n",
    "1. A way to find the best set of parameters for any function (stochastic gradient descent)\n",
    "\n",
    "This is why deep learning can do things which seem rather magical, such fantastic things. Believing that this combination of simple techniques can really solve any problem is one of the biggest steps that we find many students have to take. It seems too good to be true—surely things should be more difficult and complicated than this? Our recommendation: try it out! We just tried it on the MNIST dataset and you have seen the results. And since we are doing everything from scratch ourselves (except for calculating the gradients) you know that there is no special magic hiding behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to stop at just two linear layers. We can add as many as we want, as long as we add a nonlinearity between each pair of linear layers. As you will learn, however, the deeper the model gets, the harder it is to optimize the parameters in practice. Later in this book you will learn about some simple but brilliantly effective techniques for training deeper models.\n",
    "\n",
    "We already know that a single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers.\n",
    "\n",
    "That means that we can train the model more quickly, and it will take up less memory. In the 1990s researchers were so focused on the universal approximation theorem that very few were experimenting with more than one nonlinearity. This theoretical but not practical foundation held back the field for years. Some researchers, however, did experiment with deep models, and eventually were able to show that these models could perform much better in practice. Eventually, theoretical results were developed which showed why this happens. Today, it is extremely unusual to find anybody using a neural network with just one nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly 100% accuracy! That's a big difference compared to our simple neural net. But as you'll learn in the remainder of this book, there are just a few little tricks you need to use to get such great results from scratch yourself. You already know the key foundational pieces. (Of course, even once you know all the tricks, you'll nearly always want to work with the pre-built classes provided by PyTorch and fastai, because they save you having to think about all the little details yourself.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrate Psudo example\n",
    "We’ll use the first tensor from your dataset (`train_x[:1]`) and manually compute everything.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Setup**\n",
    "\n",
    "We’ll assume:\n",
    "1. The input tensor is `x = train_x[:1]` (a flattened image with 784 pixel values).\n",
    "2. The model has one weight vector `w` of size `(784,)` and one bias `b`.\n",
    "3. The learning rate is `lr = 0.01`.\n",
    "4. The loss function is **Binary Cross-Entropy (BCE)**, which we’ll compute manually.\n",
    "\n",
    "Let’s initialize the parameters randomly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor (flattened image)\n",
    "x = train_x[:1]  # Shape: (1, 784)\n",
    "\n",
    "# Randomly initialize weights and bias\n",
    "numpy.random.seed(42)  # For reproducibility\n",
    "w = numpy.random.randn(784)  # Shape: (784,)\n",
    "b = numpy.random.randn()     # Scalar\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# True label (e.g., 1 for digit '3', 0 for digit '7')\n",
    "y_true = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Step-by-Step Demonstration**\n",
    "\n",
    "#### **Step 1: Compute the Prediction**\n",
    "The prediction is computed as:\n",
    "$$\n",
    "\\text{logit} = w \\cdot x + b\n",
    "$$\n",
    "Then, apply the sigmoid function to get the probability:\n",
    "$$\n",
    "\\text{prediction} = \\sigma(\\text{logit}) = \\frac{1}{1 + e^{-\\text{logit}}}\n",
    "$$\n",
    "\n",
    "Here’s the manual computation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute logit (weighted sum + bias)\n",
    "logit = numpy.dot(x, w) + b  # Shape: (1,)\n",
    "\n",
    "# Apply sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + numpy.exp(-z))\n",
    "\n",
    "prediction = sigmoid(logit)  # Probability between 0 and 1\n",
    "print(f\"Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For example:\n",
    "- If `logit = 2.0`, then `sigmoid(2.0) ≈ 0.88`.\n",
    "\n",
    "#### **Step 2: Compute the Loss**\n",
    "The loss is computed using **Binary Cross-Entropy (BCE)**:\n",
    "$$\n",
    "\\text{Loss} = - \\big( y_{\\text{true}} \\cdot \\log(\\text{prediction}) + (1 - y_{\\text{true}}) \\cdot \\log(1 - \\text{prediction}) \\big)\n",
    "$$\n",
    "\n",
    "Here’s the manual computation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute BCE loss\n",
    "def binary_cross_entropy(prediction, y_true):\n",
    "    return - (y_true * np.log(prediction) + (1 - y_true) * np.log(1 - prediction))\n",
    "\n",
    "loss = binary_cross_entropy(prediction, y_true)\n",
    "print(f\"Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For example:\n",
    "- If `prediction = 0.88` and `y_true = 1.0`, then:\n",
    "  $$\n",
    "  \\text{Loss} = - \\big( 1.0 \\cdot \\log(0.88) + (1 - 1.0) \\cdot \\log(1 - 0.88) \\big) \\approx 0.127\n",
    "  $$\n",
    "\n",
    "#### **Step 3: Compute Gradients**\n",
    "To update the parameters, we need the gradients of the loss with respect to `w` and `b`. Using calculus:\n",
    "1. Gradient of loss with respect to `logit`:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial \\text{logit}} = \\text{prediction} - y_{\\text{true}}\n",
    "   $$\n",
    "2. Gradient of `logit` with respect to `w`:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{logit}}{\\partial w} = x\n",
    "   $$\n",
    "3. Gradient of `logit` with respect to `b`:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{logit}}{\\partial b} = 1\n",
    "   $$\n",
    "\n",
    "Combine these to get the gradients:\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w} = (\\text{prediction} - y_{\\text{true}}) \\cdot x\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial b} = \\text{prediction} - y_{\\text{true}}\n",
    "$$\n",
    "\n",
    "Here’s the manual computation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute gradient of loss with respect to logit\n",
    "grad_logit = prediction - y_true\n",
    "\n",
    "# Compute gradients of loss with respect to w and b\n",
    "grad_w = grad_logit * x.flatten()  # Shape: (784,)\n",
    "grad_b = grad_logit                # Scalar\n",
    "\n",
    "print(f\"Gradient of w: {grad_w}\")\n",
    "print(f\"Gradient of b: {grad_b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For example:\n",
    "- If `prediction = 0.88` and `y_true = 1.0`, then:\n",
    "  $$\n",
    "  \\text{grad\\_logit} = 0.88 - 1.0 = -0.12\n",
    "  $$\n",
    "  $$\n",
    "  \\text{grad\\_w} = -0.12 \\cdot x\n",
    "  $$\n",
    "  $$\n",
    "  \\text{grad\\_b} = -0.12\n",
    "  $$\n",
    "\n",
    "#### **Step 4: Update Parameters**\n",
    "Update the parameters using the gradients and learning rate:\n",
    "$$\n",
    "w = w - \\text{lr} \\cdot \\text{grad\\_w}\n",
    "$$\n",
    "$$\n",
    "b = b - \\text{lr} \\cdot \\text{grad\\_b}\n",
    "$$\n",
    "\n",
    "Here’s the manual computation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update weights and bias\n",
    "w = w - lr * grad_w\n",
    "b = b - lr * grad_b\n",
    "\n",
    "print(f\"Updated w: {w}\")\n",
    "print(f\"Updated b: {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For example:\n",
    "- If `grad_w = [-0.12, -0.12, ...]` and `grad_b = -0.12`, then:\n",
    "  $$\n",
    "  w = w - 0.01 \\cdot [-0.12, -0.12, ...]\n",
    "  $$\n",
    "  $$\n",
    "  b = b - 0.01 \\cdot (-0.12)\n",
    "  $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
