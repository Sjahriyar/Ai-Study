{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pytorch-transpose\"></a>\n",
    "## `.T` PyTorch Transpose in details:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **What Does `weights.T` Do?**\n",
    "\n",
    "- If `weights` is a matrix (or tensor), `.T` transposes it:\n",
    "  - Rows become columns, and columns become rows.\n",
    "  - For example:\n",
    "    ```python\n",
    "    weights = [[1, 2],\n",
    "               [3, 4]]\n",
    "    weights.T  # Result: [[1, 3],\n",
    "               #          [2, 4]]\n",
    "    ```\n",
    "\n",
    "- In PyTorch, `.T` is shorthand for `.transpose(0, 1)` (swapping the first and second dimensions).\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **Why Use `weights.T` Here?**\n",
    "\n",
    "In the expression `(train_x[0] * weights.T).sum() + bias`, the transpose ensures that the dimensions of `weights` align correctly with `train_x[0]` for matrix multiplication.\n",
    "\n",
    "##### Dimensions Breakdown:\n",
    "- `train_x[0]`: A single data point (e.g., an image flattened into a vector). Shape: `(784,)` (1D tensor with 784 features).\n",
    "- `weights`: The learnable parameters for the model. Shape: `(784, 1)` (2D tensor with 784 rows and 1 column).\n",
    "\n",
    "If you directly multiply `train_x[0]` (shape `(784,)`) with `weights` (shape `(784, 1)`), the dimensions won’t align properly for element-wise multiplication or dot product.\n",
    "\n",
    "By using `weights.T`, the shape becomes `(1, 784)`, which allows the computation to proceed correctly.\n",
    "\n",
    "1. **`train_x[0]`**:\n",
    "   - A single data point (flattened image). Shape: `(784,)`.\n",
    "\n",
    "2. **`weights.T`**:\n",
    "   - Transposed weights. Shape: `(1, 784)`.\n",
    "\n",
    "3. **`train_x[0] * weights.T`**:\n",
    "   - Element-wise multiplication between `train_x[0]` and `weights.T`.\n",
    "   - This computes the weighted contribution of each feature.\n",
    "\n",
    "4. **`.sum()`**:\n",
    "   - Sums up all the weighted contributions to produce a single scalar value.\n",
    "\n",
    "5. **`+ bias`**:\n",
    "   - Adds the bias term to shift the result.\n",
    "\n",
    "The final output is the prediction for the input `train_x[0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sgd-illustrate-with-code\"></a>\n",
    "## SGD Illustrate with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let’s assume the image has only 4 pixels instead of 784 (this makes it easier to visualize).\n",
    "```python\n",
    "one_image = [0.1, 0.5, 0.3, 0.9]  # Pixel values of the image (scaled between 0 and 1).\n",
    "\n",
    "#The weights represent how important each pixel is for making a prediction.\n",
    "# Since the input has 4 pixels, the weights will also have 4 values (one for each pixel).\n",
    "weights = [[0.2],  # Weight for pixel 1\n",
    "           [0.4],  # Weight for pixel 2\n",
    "           [-0.1], # Weight for pixel 3\n",
    "           [0.3]]  # Weight for pixel 4\n",
    "\n",
    "#To align the dimensions for element-wise multiplication, we transpose the weights.\n",
    "# Transposing converts weights from shape (4, 1) to (1, 4).\n",
    "weights_transpose = [[0.2, 0.4, -0.1, 0.3]]  # Shape: (1, 4)\n",
    "\n",
    "# Multiply each pixel value in one_image by the corresponding weight in weights_transpose.\n",
    "element_wise_product = [0.1 * 0.2,  # Pixel 1 × Weight 1\n",
    "                        0.5 * 0.4,  # Pixel 2 × Weight 2\n",
    "                        0.3 * -0.1, # Pixel 3 × Weight 3\n",
    "                        0.9 * 0.3]  # Pixel 4 × Weight 4\n",
    "\n",
    "# Result\n",
    "element_wise_product = [0.02, 0.2, -0.03, 0.27]\n",
    "\n",
    "# Add up all the values in the element_wise_product to get a single scalar value.\n",
    "weighted_sum = 0.02 + 0.2 + (-0.03) + 0.27 = 0.46\n",
    "\n",
    "# The bias is a single number that shifts the result up or down.\n",
    "bias = 0.1\n",
    "prediction = weighted_sum + bias = 0.46 + 0.1 = 0.56\n",
    "\n",
    "# Final Prediction\n",
    "one_image_prediction = 0.56\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-transformation-matrix-multiplication\"></a>\n",
    "## Linear Transformation: Matrix Multiplication `batch @ weights + bias`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation, `batch@weights + bias`, is one of the two fundamental equations of any neural network. The second one is **Activation Function**.\n",
    "This expression represents a **linear transformation** commonly used in machine learning, especially in neural networks. Here's what each part means:\n",
    "\n",
    "1. **`batch`**:\n",
    "   - This is a **matrix** (or tensor) containing multiple input data points. Each row corresponds to one data point, and each column corresponds to a feature (e.g., pixel values, sensor readings, etc.).\n",
    "   - Shape: Typically `(n, m)`, where:\n",
    "     - `n` = number of data points in the batch,\n",
    "     - `m` = number of features per data point.\n",
    "\n",
    "2. **`weights`**:\n",
    "   - This is a **matrix** of learnable parameters that the model uses to transform the input data.\n",
    "   - Shape: Typically `(m, p)`, where:\n",
    "     - `m` = number of input features (must match the second dimension of `batch`),\n",
    "     - `p` = number of output features (e.g., neurons in the next layer).\n",
    "\n",
    "3. **`@`**:\n",
    "   - The `@` operator performs **matrix multiplication** between `batch` and `weights`.\n",
    "   - Resulting shape: `(n, p)` (number of data points × number of output features).\n",
    "\n",
    "4. **`bias`**:\n",
    "   - This is a **vector** (or 1D tensor) of learnable parameters added to the result of the matrix multiplication.\n",
    "   - Shape: `(p,)` (must match the number of output features).\n",
    "\n",
    "5. **`+ bias`**:\n",
    "   - After the matrix multiplication, the `bias` is added element-wise to each row of the resulting matrix.\n",
    "\n",
    "### Why Is This Important?\n",
    "\n",
    "The operation `batch @ weights + bias` is the core of a **linear layer** in neural networks. It transforms the input data into a new representation by applying a weighted sum of the inputs and adding a bias term. This transformation is fundamental for tasks like classification, regression, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Calculation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [[1, 2, 3],    # Data point 1\n",
    "        [4, 5, 6]]    # Data point 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Shape: `(2, 3)` (2 data points, 3 features).\n",
    "\n",
    "- A **weights** matrix that maps 3 input features to 2 output features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[0.1, 0.2],  # Weights for output feature 1\n",
    "            [0.3, 0.4],  # Weights for output feature 2\n",
    "            [0.5, 0.6]]  # Weights for output feature 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Shape: `(3, 2)` (3 input features, 2 output features).\n",
    "\n",
    "- A **bias** vector for the 2 output features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = [0.1, 0.2]  # Bias for output feature 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape: `(2,)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Define the Inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Side note:** A feature is like a descriptive attribute or dimension of your data. For example: If we're working with images, features could be pixel values.\n",
    "\n",
    "Let’s say we have:\n",
    "- A **batch** of 2 data points, each with 3 features:\n",
    "  ```python\n",
    "  batch = [[1, 2, 3],    # Data point 1\n",
    "           [4, 5, 6]]    # Data point 2\n",
    "  ```\n",
    "  Shape: `(2, 3)` (2 data points, 3 features).\n",
    "\n",
    "- A **weights** matrix that maps 3 input features to 2 output features:\n",
    "  ```python\n",
    "  weights = [[0.1, 0.2],  # Weights for output feature 1\n",
    "             [0.3, 0.4],  # Weights for output feature 2\n",
    "             [0.5, 0.6]]  # Weights for output feature 3\n",
    "  ```\n",
    "  Shape: `(3, 2)` (3 input features, 2 output features).\n",
    "\n",
    "- A **bias** vector for the 2 output features:\n",
    "  ```python\n",
    "  bias = [0.1, 0.2]  # Bias for output feature 1 and 2\n",
    "  ```\n",
    "  Shape: `(2,)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Perform Matrix Multiplication (`batch @ weights`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the matrix multiplication between `batch` and `weights`. For each data point, this calculates a weighted sum of the input features.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "\\text{result}[i, j] = \\sum_{k} \\text{batch}[i, k] \\cdot \\text{weights}[k, j]\n",
    "$$\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "result = batch @ weights\n",
    "```\n",
    "\n",
    "Step-by-step:\n",
    "1. First data point (`[1, 2, 3]`) multiplied by `weights`:\n",
    "   $$\n",
    "   [1 \\cdot 0.1 + 2 \\cdot 0.3 + 3 \\cdot 0.5, \\quad 1 \\cdot 0.2 + 2 \\cdot 0.4 + 3 \\cdot 0.6]\n",
    "   = [2.2, 2.8]\n",
    "   $$\n",
    "\n",
    "2. Second data point (`[4, 5, 6]`) multiplied by `weights`:\n",
    "   $$\n",
    "   [4 \\cdot 0.1 + 5 \\cdot 0.3 + 6 \\cdot 0.5, \\quad 4 \\cdot 0.2 + 5 \\cdot 0.4 + 6 \\cdot 0.6]\n",
    "   = [4.9, 6.4]\n",
    "   $$\n",
    "\n",
    "So, the result of `batch @ weights` is:\n",
    "```python\n",
    "[[2.2, 2.8],  # Output for data point 1\n",
    " [4.9, 6.4]]  # Output for data point 2\n",
    "```\n",
    "Shape: `(2, 2)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 3: Add the Bias (`+ bias`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add the `bias` vector `[0.1, 0.2]` to each row of the result.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "\\text{final}[i, j] = \\text{result}[i, j] + \\text{bias}[j]\n",
    "$$\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "final = result + bias\n",
    "```\n",
    "\n",
    "Step-by-step:\n",
    "1. Add bias to the first row:\n",
    "   $$\n",
    "   [2.2 + 0.1, \\quad 2.8 + 0.2] = [2.3, 3.0]\n",
    "   $$\n",
    "\n",
    "2. Add bias to the second row:\n",
    "   $$\n",
    "   [4.9 + 0.1, \\quad 6.4 + 0.2] = [5.0, 6.6]\n",
    "   $$\n",
    "\n",
    "So, the final result is:\n",
    "```python\n",
    "[[2.3, 3.0],  # Final output for data point 1\n",
    " [5.0, 6.6]]  # Final output for data point 2\n",
    " ```\n",
    "\n",
    " Shape: `(2, 2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The operation `batch @ weights + bias` transforms the input data into a new representation by applying a weighted sum and adding a bias. For our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "batch = [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "weights = [[0.1, 0.2],\n",
    "           [0.3, 0.4],\n",
    "           [0.5, 0.6]]\n",
    "bias = [0.1, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"binary-cross-entropy\"></a>\n",
    "## Binary Cross-Entropy (BCE) loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy (BCE) loss is a way to measure how well a model’s predictions match the true labels in binary classification problems (e.g., predicting whether an image is a 3 or a 7). It penalizes the model when its predictions are far from the correct answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How Does It Work?**\n",
    "\n",
    "1. **True Labels**:\n",
    "   - Each example has a true label, either `0` or `1`.\n",
    "\n",
    "2. **Predictions**:\n",
    "   - The model outputs a probability between `0` and `1` for each example (e.g., `0.9` means the model is 90% confident the label is `1`).\n",
    "\n",
    "3. **Penalty**:\n",
    "   - If the true label is `1`, BCE penalizes predictions far from `1`.\n",
    "   - If the true label is `0`, BCE penalizes predictions far from `0`.\n",
    "\n",
    "4. **Goal**:\n",
    "   - Minimize the BCE loss so the model makes better predictions.\n",
    "\n",
    "### **Formula**\n",
    "\n",
    "The BCE loss for one example is:\n",
    "$$\n",
    "\\text{Loss} = - \\big( t \\cdot \\log(p) + (1 - t) \\cdot \\log(1 - p) \\big)\n",
    "$$\n",
    "Where:\n",
    "- $t$: True label (`0` or `1`),\n",
    "- $p$: Predicted probability (between `0` and `1`).\n",
    "\n",
    "For a batch of examples, the losses are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Code Example**\n",
    "\n",
    "Here’s a simple example using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss: 0.1976\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# True labels (0 or 1)\n",
    "targets = torch.tensor([1, 0, 1, 0], dtype=torch.float32)\n",
    "\n",
    "# Model predictions (probabilities between 0 and 1)\n",
    "predictions = torch.tensor([0.9, 0.2, 0.7, 0.1], dtype=torch.float32)\n",
    "\n",
    "# Compute BCE loss\n",
    "loss = F.binary_cross_entropy(predictions, targets)\n",
    "print(f\"BCE Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pytorch-dataset\"></a>\n",
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, a `Dataset` is a class that helps you manage your data. It provides a way to:\n",
    "1. Store your input data (`x`) and labels (`y`).\n",
    "2. Access individual data points (e.g., one image and its label) when needed.\n",
    "\n",
    "When you’re training a machine learning model, you typically use a `Dataset` to feed data into the model in small batches.\n",
    "\n",
    "### What Does \"Return a Tuple of `(x, y)`\" Mean?\n",
    "\n",
    "A `Dataset` in PyTorch is required to return two things for each data point:\n",
    "- `x`: The input data (e.g., an image or feature vector).\n",
    "- `y`: The corresponding label (e.g., the category or target value).\n",
    "\n",
    "These are returned as a **tuple** `(x, y)`.\n",
    "\n",
    "For example:\n",
    "- If you have an image of a handwritten digit `3`, then:\n",
    "  - `x` might be the pixel values of the image.\n",
    "  - `y` might be the label `1` (indicating it’s a `3`).\n",
    "\n",
    "### What Does \"When Indexed\" Mean?\n",
    "\n",
    "The phrase **\"when indexed\"** refers to how you access individual elements from the `Dataset`. In Python, indexing means accessing an element by its position using square brackets (`[]`).\n",
    "\n",
    "For example:\n",
    "```python\n",
    "dataset[0]  # Access the first element in the dataset\n",
    "```\n",
    "\n",
    "\n",
    "In PyTorch, when you index a `Dataset` (e.g., `dataset[0]`), it must return a tuple `(x, y)` containing:\n",
    "- `x`: The input data for that specific index.\n",
    "- `y`: The corresponding label for that specific index.\n",
    "\n",
    "### Example: How Indexing Works in a Dataset\n",
    "\n",
    "Let’s say we have a simple `Dataset` with three images and their labels:\n",
    "\n",
    "| Index | Image (`x`)       | Label (`y`) |\n",
    "|-------|-------------------|-------------|\n",
    "| 0     | Image of a `3`    | 1           |\n",
    "| 1     | Image of a `7`    | 0           |\n",
    "| 2     | Image of another `3` | 1       |\n",
    "\n",
    "If you index this dataset:\n",
    "```python\n",
    "dataset[0]  # Returns (image_of_3, 1)\n",
    "dataset[1]  # Returns (image_of_7, 0)\n",
    "dataset[2]  # Returns (another_image_of_3, 1)\n",
    "```\n",
    "Each time you index the dataset, it gives you a tuple `(x, y)` for the corresponding data point.\n",
    "\n",
    "### Why Is This Important?\n",
    "\n",
    "When training a model, PyTorch uses a `DataLoader` to iterate over the dataset in batches. The `DataLoader` relies on the fact that the `Dataset` returns `(x, y)` when indexed. This ensures that:\n",
    "1. The input data (`x`) and labels (`y`) are paired correctly.\n",
    "2. The data can be fed into the model for training or evaluation.\n",
    "\n",
    "In short: **\"When indexed\" means accessing a specific data point, and the `Dataset` must return the input data and its label as a tuple `(x, y)`.** 😊\n",
    "a = torch.tensor([[1.,2.,3.], [4.,5.,6.]])\n",
    "b = torch.tensor([[74.,7.,8.], [9.,19.,22.]])\n",
    "cat = torch.cat((a,b))\n",
    "cat.view(-1, 2*3)\n",
    "\n",
    "train_y = torch.tensor([1]*len(a) + [0]*len(b)).unsqueeze(1)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-model\"></a>\n",
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **linear model** is a type of machine learning model that assumes the relationship between the input features (e.g., $x$) and the output (e.g., $y$) is a straight line. It works by finding the best-fitting line through the data.\n",
    "\n",
    "#### **Key Idea**\n",
    "The formula for a linear model is:\n",
    "$$\n",
    "y = w \\cdot x + b\n",
    "$$\n",
    "Where:\n",
    "- $x$: The input feature(s) (e.g., house size, temperature, etc.).\n",
    "- $w$: The weight (or slope), which determines how much each input contributes to the output.\n",
    "- $b$: The bias (or intercept), which shifts the line up or down.\n",
    "- $y$: The predicted output (e.g., house price, classification score).\n",
    "\n",
    "#### **What Does It Do?**\n",
    "- It predicts an output ($y$) based on the input ($x$).\n",
    "- For example:\n",
    "  - Predicting house prices: $y = 100,000 \\cdot (\\text{size}) + 50,000$.\n",
    "  - Classifying images: $y = (\\text{pixel values} \\cdot \\text{weights}) + \\text{bias}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **When Do We Use Linear Layers?**\n",
    "Linear layers (also called fully connected layers ) are used when:\n",
    "\n",
    "The input data is already in a flat, vectorized form (e.g., tabular data or flattened images).\n",
    "The problem requires learning relationships between features in a dense, unstructured way .\n",
    "You’re building simple feedforward neural networks (e.g., for regression or classification tasks with small datasets).\n",
    "\n",
    "For example:\n",
    "\n",
    "In your code, the Linear layers are used because the input images are flattened into vectors (28*28 = 784), and the network processes them as dense feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image id=\"linear-model-graph\" src=\"./images/linear_model_example.png\" width=350 height=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **What Does the [Graph](#linear-model) Show?**\n",
    "\n",
    "1. **Blue Dots**:\n",
    "   - These are the data points generated using the formula $y = 2.5 \\cdot x + 1.0 + \\text{noise}$.\n",
    "   - The noise makes the points scatter around the true line.\n",
    "\n",
    "2. **Red Line**:\n",
    "   - This is the best-fitting line calculated using the linear model.\n",
    "   - The equation of the line is displayed in the legend (e.g., $y = 2.49x + 1.05$).\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - The red line represents the linear model's prediction.\n",
    "   - For any new $x$, you can use the equation of the line to predict $y$.\n",
    "\n",
    "### **Why Use a Linear Model?**\n",
    "\n",
    "1. **Simple and Interpretable**:\n",
    "   - You can easily understand how each input affects the output (via the weights).\n",
    "\n",
    "2. **Fast to Train**:\n",
    "   - Linear models are computationally efficient and work well for small datasets.\n",
    "\n",
    "3. **Good for Linear Relationships**:\n",
    "   - If the data follows a straight-line pattern, a linear model is perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nr-of-neurons\"></a>\n",
    "# Number of neurons (or units) in the hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below: \n",
    "\n",
    "```python\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "```\n",
    "We have are using Image set of FashionMNIST which includes images of 28x28 pixels meaning in total 784 features.\n",
    "We are then passing the number `512` a **hyperparameter** to the `Linear` module's `out_features` arguement.\n",
    " \n",
    "### **Why Use `512`?**\n",
    "\n",
    "1. **Arbitrary Choice (But Reasonable)**:\n",
    "   - The number `512` is not derived from any specific formula or rule—it’s chosen by the developer based on experience, experimentation, or common practices.\n",
    "   - `512` is a common choice for hidden layer sizes because it strikes a balance between:\n",
    "     - **Expressiveness**: More neurons allow the network to learn more complex patterns.\n",
    "     - **Computational Feasibility**: Too many neurons can make the model slower and require more memory.\n",
    "\n",
    "2. **Hidden Layer Size**:\n",
    "   - Hidden layers are where the network learns intermediate representations of the data. A larger hidden layer size (like `512`) allows the network to capture more nuanced features but also increases computational cost.\n",
    "\n",
    "3. **Power of Two**:\n",
    "   - Numbers like `512` (a power of 2: $2^9$) are often used in deep learning because they align well with hardware optimizations (e.g., GPUs process data in chunks that are powers of 2).\n",
    "\n",
    "### **Where Does This Number Come From?**\n",
    "\n",
    "The number `512` doesn’t come from the data itself but is determined by several factors:\n",
    "\n",
    "#### 1. **Problem Complexity**:\n",
    "   - For simple problems (e.g., linear regression), you might use smaller hidden layers (e.g., `16`, `32`).\n",
    "   - For complex problems (e.g., image classification), larger hidden layers (e.g., `128`, `256`, `512`) are often needed to capture intricate patterns.\n",
    "\n",
    "#### 2. **Input Size**:\n",
    "   - The input size (`28*28 = 784` in your case) gives a rough idea of how large the hidden layers should be. A common heuristic is to make the hidden layers smaller than the input size but still large enough to represent the data effectively.\n",
    "\n",
    "#### 3. **Trial and Error**:\n",
    "   - Developers often experiment with different hidden layer sizes to find what works best for their specific problem. For example:\n",
    "     - Start with `128`, then try `256`, `512`, etc., until the model performs well without overfitting.\n",
    "\n",
    "#### 4. **Common Practices**:\n",
    "   - In many deep learning applications, hidden layer sizes like `128`, `256`, or `512` are standard choices because they work well across a variety of tasks.\n",
    "\n",
    "\n",
    "### **What Happens If You Change `512`?**\n",
    "\n",
    "- **Smaller Hidden Layers (e.g., `128`)**:\n",
    "  - The network will have fewer parameters, making it faster and less prone to overfitting.\n",
    "  - However, it might struggle to learn complex patterns if the problem is too difficult.\n",
    "\n",
    "- **Larger Hidden Layers (e.g., `1024`)**:\n",
    "  - The network will have more capacity to learn complex patterns.\n",
    "  - However, it will also require more memory and computation, and it might overfit if the dataset is small.\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "1. **What Is `512`?**\n",
    "   - It’s the number of neurons in the hidden layers of your neural network.\n",
    "\n",
    "2. **Why Use `512`?**\n",
    "   - It’s a reasonable choice based on common practices, computational feasibility, and the complexity of the problem.\n",
    "\n",
    "3. **Where Does It Come From?**\n",
    "   - It’s a hyperparameter chosen by the developer, not derived from the data. It’s based on problem complexity, input size, and experimentation.\n",
    "\n",
    "4. **Can You Change It?**\n",
    "   - Yes! You can experiment with different values (e.g., `128`, `256`, `1024`) to see what works best for your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
