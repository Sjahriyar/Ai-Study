{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pytorch-transpose\"></a>\n",
    "## `.T` PyTorch Transpose in details:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **What Does `weights.T` Do?**\n",
    "\n",
    "- If `weights` is a matrix (or tensor), `.T` transposes it:\n",
    "  - Rows become columns, and columns become rows.\n",
    "  - For example:\n",
    "    ```python\n",
    "    weights = [[1, 2],\n",
    "               [3, 4]]\n",
    "    weights.T  # Result: [[1, 3],\n",
    "               #          [2, 4]]\n",
    "    ```\n",
    "\n",
    "- In PyTorch, `.T` is shorthand for `.transpose(0, 1)` (swapping the first and second dimensions).\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **Why Use `weights.T` Here?**\n",
    "\n",
    "In the expression `(train_x[0] * weights.T).sum() + bias`, the transpose ensures that the dimensions of `weights` align correctly with `train_x[0]` for matrix multiplication.\n",
    "\n",
    "##### Dimensions Breakdown:\n",
    "- `train_x[0]`: A single data point (e.g., an image flattened into a vector). Shape: `(784,)` (1D tensor with 784 features).\n",
    "- `weights`: The learnable parameters for the model. Shape: `(784, 1)` (2D tensor with 784 rows and 1 column).\n",
    "\n",
    "If you directly multiply `train_x[0]` (shape `(784,)`) with `weights` (shape `(784, 1)`), the dimensions wonâ€™t align properly for element-wise multiplication or dot product.\n",
    "\n",
    "By using `weights.T`, the shape becomes `(1, 784)`, which allows the computation to proceed correctly.\n",
    "\n",
    "1. **`train_x[0]`**:\n",
    "   - A single data point (flattened image). Shape: `(784,)`.\n",
    "\n",
    "2. **`weights.T`**:\n",
    "   - Transposed weights. Shape: `(1, 784)`.\n",
    "\n",
    "3. **`train_x[0] * weights.T`**:\n",
    "   - Element-wise multiplication between `train_x[0]` and `weights.T`.\n",
    "   - This computes the weighted contribution of each feature.\n",
    "\n",
    "4. **`.sum()`**:\n",
    "   - Sums up all the weighted contributions to produce a single scalar value.\n",
    "\n",
    "5. **`+ bias`**:\n",
    "   - Adds the bias term to shift the result.\n",
    "\n",
    "The final output is the prediction for the input `train_x[0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sgd-illustrate-with-code\"></a>\n",
    "## SGD Illustrate with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, letâ€™s assume the image has only 4 pixels instead of 784 (this makes it easier to visualize).\n",
    "```python\n",
    "one_image = [0.1, 0.5, 0.3, 0.9]  # Pixel values of the image (scaled between 0 and 1).\n",
    "\n",
    "#The weights represent how important each pixel is for making a prediction.\n",
    "# Since the input has 4 pixels, the weights will also have 4 values (one for each pixel).\n",
    "weights = [[0.2],  # Weight for pixel 1\n",
    "           [0.4],  # Weight for pixel 2\n",
    "           [-0.1], # Weight for pixel 3\n",
    "           [0.3]]  # Weight for pixel 4\n",
    "\n",
    "#To align the dimensions for element-wise multiplication, we transpose the weights.\n",
    "# Transposing converts weights from shape (4, 1) to (1, 4).\n",
    "weights_transpose = [[0.2, 0.4, -0.1, 0.3]]  # Shape: (1, 4)\n",
    "\n",
    "# Multiply each pixel value in one_image by the corresponding weight in weights_transpose.\n",
    "element_wise_product = [0.1 * 0.2,  # Pixel 1 Ã— Weight 1\n",
    "                        0.5 * 0.4,  # Pixel 2 Ã— Weight 2\n",
    "                        0.3 * -0.1, # Pixel 3 Ã— Weight 3\n",
    "                        0.9 * 0.3]  # Pixel 4 Ã— Weight 4\n",
    "\n",
    "# Result\n",
    "element_wise_product = [0.02, 0.2, -0.03, 0.27]\n",
    "\n",
    "# Add up all the values in the element_wise_product to get a single scalar value.\n",
    "weighted_sum = 0.02 + 0.2 + (-0.03) + 0.27 = 0.46\n",
    "\n",
    "# The bias is a single number that shifts the result up or down.\n",
    "bias = 0.1\n",
    "prediction = weighted_sum + bias = 0.46 + 0.1 = 0.56\n",
    "\n",
    "# Final Prediction\n",
    "one_image_prediction = 0.56\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-transformation-matrix-multiplication\"></a>\n",
    "## Linear Transformation: Matrix Multiplication `batch @ weights + bias`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation, `batch@weights + bias`, is one of the two fundamental equations of any neural network. The second one is **Activation Function**.\n",
    "This expression represents a **linear transformation** commonly used in machine learning, especially in neural networks. Here's what each part means:\n",
    "\n",
    "1. **`batch`**:\n",
    "   - This is a **matrix** (or tensor) containing multiple input data points. Each row corresponds to one data point, and each column corresponds to a feature (e.g., pixel values, sensor readings, etc.).\n",
    "   - Shape: Typically `(n, m)`, where:\n",
    "     - `n` = number of data points in the batch,\n",
    "     - `m` = number of features per data point.\n",
    "\n",
    "2. **`weights`**:\n",
    "   - This is a **matrix** of learnable parameters that the model uses to transform the input data.\n",
    "   - Shape: Typically `(m, p)`, where:\n",
    "     - `m` = number of input features (must match the second dimension of `batch`),\n",
    "     - `p` = number of output features (e.g., neurons in the next layer).\n",
    "\n",
    "3. **`@`**:\n",
    "   - The `@` operator performs **matrix multiplication** between `batch` and `weights`.\n",
    "   - Resulting shape: `(n, p)` (number of data points Ã— number of output features).\n",
    "\n",
    "4. **`bias`**:\n",
    "   - This is a **vector** (or 1D tensor) of learnable parameters added to the result of the matrix multiplication.\n",
    "   - Shape: `(p,)` (must match the number of output features).\n",
    "\n",
    "5. **`+ bias`**:\n",
    "   - After the matrix multiplication, the `bias` is added element-wise to each row of the resulting matrix.\n",
    "\n",
    "### Why Is This Important?\n",
    "\n",
    "The operation `batch @ weights + bias` is the core of a **linear layer** in neural networks. It transforms the input data into a new representation by applying a weighted sum of the inputs and adding a bias term. This transformation is fundamental for tasks like classification, regression, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Calculation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [[1, 2, 3],    # Data point 1\n",
    "        [4, 5, 6]]    # Data point 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Shape: `(2, 3)` (2 data points, 3 features).\n",
    "\n",
    "- A **weights** matrix that maps 3 input features to 2 output features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[0.1, 0.2],  # Weights for output feature 1\n",
    "            [0.3, 0.4],  # Weights for output feature 2\n",
    "            [0.5, 0.6]]  # Weights for output feature 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Shape: `(3, 2)` (3 input features, 2 output features).\n",
    "\n",
    "- A **bias** vector for the 2 output features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = [0.1, 0.2]  # Bias for output feature 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape: `(2,)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Define the Inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Side note:** A feature is like a descriptive attribute or dimension of your data. For example: If we're working with images, features could be pixel values.\n",
    "\n",
    "Letâ€™s say we have:\n",
    "- A **batch** of 2 data points, each with 3 features:\n",
    "  ```python\n",
    "  batch = [[1, 2, 3],    # Data point 1\n",
    "           [4, 5, 6]]    # Data point 2\n",
    "  ```\n",
    "  Shape: `(2, 3)` (2 data points, 3 features).\n",
    "\n",
    "- A **weights** matrix that maps 3 input features to 2 output features:\n",
    "  ```python\n",
    "  weights = [[0.1, 0.2],  # Weights for output feature 1\n",
    "             [0.3, 0.4],  # Weights for output feature 2\n",
    "             [0.5, 0.6]]  # Weights for output feature 3\n",
    "  ```\n",
    "  Shape: `(3, 2)` (3 input features, 2 output features).\n",
    "\n",
    "- A **bias** vector for the 2 output features:\n",
    "  ```python\n",
    "  bias = [0.1, 0.2]  # Bias for output feature 1 and 2\n",
    "  ```\n",
    "  Shape: `(2,)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Perform Matrix Multiplication (`batch @ weights`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the matrix multiplication between `batch` and `weights`. For each data point, this calculates a weighted sum of the input features.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "\\text{result}[i, j] = \\sum_{k} \\text{batch}[i, k] \\cdot \\text{weights}[k, j]\n",
    "$$\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "result = batch @ weights\n",
    "```\n",
    "\n",
    "Step-by-step:\n",
    "1. First data point (`[1, 2, 3]`) multiplied by `weights`:\n",
    "   $$\n",
    "   [1 \\cdot 0.1 + 2 \\cdot 0.3 + 3 \\cdot 0.5, \\quad 1 \\cdot 0.2 + 2 \\cdot 0.4 + 3 \\cdot 0.6]\n",
    "   = [2.2, 2.8]\n",
    "   $$\n",
    "\n",
    "2. Second data point (`[4, 5, 6]`) multiplied by `weights`:\n",
    "   $$\n",
    "   [4 \\cdot 0.1 + 5 \\cdot 0.3 + 6 \\cdot 0.5, \\quad 4 \\cdot 0.2 + 5 \\cdot 0.4 + 6 \\cdot 0.6]\n",
    "   = [4.9, 6.4]\n",
    "   $$\n",
    "\n",
    "So, the result of `batch @ weights` is:\n",
    "```python\n",
    "[[2.2, 2.8],  # Output for data point 1\n",
    " [4.9, 6.4]]  # Output for data point 2\n",
    "```\n",
    "Shape: `(2, 2)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 3: Add the Bias (`+ bias`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add the `bias` vector `[0.1, 0.2]` to each row of the result.\n",
    "\n",
    "Mathematically:\n",
    "$$\n",
    "\\text{final}[i, j] = \\text{result}[i, j] + \\text{bias}[j]\n",
    "$$\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "final = result + bias\n",
    "```\n",
    "\n",
    "Step-by-step:\n",
    "1. Add bias to the first row:\n",
    "   $$\n",
    "   [2.2 + 0.1, \\quad 2.8 + 0.2] = [2.3, 3.0]\n",
    "   $$\n",
    "\n",
    "2. Add bias to the second row:\n",
    "   $$\n",
    "   [4.9 + 0.1, \\quad 6.4 + 0.2] = [5.0, 6.6]\n",
    "   $$\n",
    "\n",
    "So, the final result is:\n",
    "```python\n",
    "[[2.3, 3.0],  # Final output for data point 1\n",
    " [5.0, 6.6]]  # Final output for data point 2\n",
    " ```\n",
    "\n",
    " Shape: `(2, 2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The operation `batch @ weights + bias` transforms the input data into a new representation by applying a weighted sum and adding a bias. For our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "\n",
    "batch = [[1, 2, 3],\n",
    "         [4, 5, 6]]\n",
    "weights = [[0.1, 0.2],\n",
    "           [0.3, 0.4],\n",
    "           [0.5, 0.6]]\n",
    "bias = [0.1, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pytorch-dataset\"></a>\n",
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, a `Dataset` is a class that helps you manage your data. It provides a way to:\n",
    "1. Store your input data (`x`) and labels (`y`).\n",
    "2. Access individual data points (e.g., one image and its label) when needed.\n",
    "\n",
    "When youâ€™re training a machine learning model, you typically use a `Dataset` to feed data into the model in small batches.\n",
    "\n",
    "### What Does \"Return a Tuple of `(x, y)`\" Mean?\n",
    "\n",
    "A `Dataset` in PyTorch is required to return two things for each data point:\n",
    "- `x`: The input data (e.g., an image or feature vector).\n",
    "- `y`: The corresponding label (e.g., the category or target value).\n",
    "\n",
    "These are returned as a **tuple** `(x, y)`.\n",
    "\n",
    "For example:\n",
    "- If you have an image of a handwritten digit `3`, then:\n",
    "  - `x` might be the pixel values of the image.\n",
    "  - `y` might be the label `1` (indicating itâ€™s a `3`).\n",
    "\n",
    "### What Does \"When Indexed\" Mean?\n",
    "\n",
    "The phrase **\"when indexed\"** refers to how you access individual elements from the `Dataset`. In Python, indexing means accessing an element by its position using square brackets (`[]`).\n",
    "\n",
    "For example:\n",
    "```python\n",
    "dataset[0]  # Access the first element in the dataset\n",
    "```\n",
    "\n",
    "\n",
    "In PyTorch, when you index a `Dataset` (e.g., `dataset[0]`), it must return a tuple `(x, y)` containing:\n",
    "- `x`: The input data for that specific index.\n",
    "- `y`: The corresponding label for that specific index.\n",
    "\n",
    "### Example: How Indexing Works in a Dataset\n",
    "\n",
    "Letâ€™s say we have a simple `Dataset` with three images and their labels:\n",
    "\n",
    "| Index | Image (`x`)       | Label (`y`) |\n",
    "|-------|-------------------|-------------|\n",
    "| 0     | Image of a `3`    | 1           |\n",
    "| 1     | Image of a `7`    | 0           |\n",
    "| 2     | Image of another `3` | 1       |\n",
    "\n",
    "If you index this dataset:\n",
    "```python\n",
    "dataset[0]  # Returns (image_of_3, 1)\n",
    "dataset[1]  # Returns (image_of_7, 0)\n",
    "dataset[2]  # Returns (another_image_of_3, 1)\n",
    "```\n",
    "Each time you index the dataset, it gives you a tuple `(x, y)` for the corresponding data point.\n",
    "\n",
    "### Why Is This Important?\n",
    "\n",
    "When training a model, PyTorch uses a `DataLoader` to iterate over the dataset in batches. The `DataLoader` relies on the fact that the `Dataset` returns `(x, y)` when indexed. This ensures that:\n",
    "1. The input data (`x`) and labels (`y`) are paired correctly.\n",
    "2. The data can be fed into the model for training or evaluation.\n",
    "\n",
    "In short: **\"When indexed\" means accessing a specific data point, and the `Dataset` must return the input data and its label as a tuple `(x, y)`.** ðŸ˜Š\n",
    "a = torch.tensor([[1.,2.,3.], [4.,5.,6.]])\n",
    "b = torch.tensor([[74.,7.,8.], [9.,19.,22.]])\n",
    "cat = torch.cat((a,b))\n",
    "cat.view(-1, 2*3)\n",
    "\n",
    "train_y = torch.tensor([1]*len(a) + [0]*len(b)).unsqueeze(1)\n",
    "train_y"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
